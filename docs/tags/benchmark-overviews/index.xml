<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Benchmark Overviews on arduin.io</title>
    <link>https://arduin.io/tags/benchmark-overviews/</link>
    <description>Recent content in Benchmark Overviews on arduin.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 16 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://arduin.io/tags/benchmark-overviews/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MMLU Benchmark Overview</title>
      <link>https://arduin.io/blog/mmlu-overview/</link>
      <pubDate>Sat, 16 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://arduin.io/blog/mmlu-overview/</guid>
      <description>The Massive Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2021) is widely used to demonstrate state-of-the-art language model capabilities. Anthropic’s Claude 3, Google’s Gemini and OpenAI’s GPT-4 models were all introduced alongside prominently placed MMLU results. This publicity makes MMLU one of the most prominently discussed benchmarks for language models. Despite the benchmark’s prominence, the exact model capabilities evaluated and evaluation methods are less widely known. In this blog post, I aim to give a short overview of the MMLU benchmark.</description>
    </item>
    
  </channel>
</rss>
