<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Teaching on arduin.io</title>
    <link>https://arduin.io/tags/teaching/</link>
    <description>Recent content in Teaching on arduin.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://arduin.io/tags/teaching/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GPT illustrated: from high-level diagram to vectors and code</title>
      <link>https://arduin.io/blog/gpt-illustrated/</link>
      <pubDate>Sun, 10 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://arduin.io/blog/gpt-illustrated/</guid>
      <description>There are many excellent explanations and illustrations of the original transformer (Vaswani&amp;#32; &amp;#32;et&amp;#32;al.,&amp;#32;2017 Vaswani,&amp;#32; A.,&amp;#32; Shazeer,&amp;#32; N.,&amp;#32; Parmar,&amp;#32; N.,&amp;#32; Uszkoreit,&amp;#32; J.,&amp;#32; Jones,&amp;#32; L.,&amp;#32; Gomez,&amp;#32; A.,&amp;#32; Kaiser,&amp;#32; \.&amp;#32;&amp;amp;&amp;#32;Polosukhin,&amp;#32; I. &amp;#32; (2017). &amp;#32;Attention is all you need. Advances in neural information processing systems,&amp;#32;30. ) and generative pre-trained transformer (GPT) (Radford&amp;#32; &amp;#32;et&amp;#32;al.,&amp;#32;2018 Radford,&amp;#32; A.,&amp;#32; Narasimhan,&amp;#32; K.,&amp;#32; Salimans,&amp;#32; T.&amp;#32;&amp;amp;&amp;#32;Sutskever,&amp;#32; I. &amp;#32; (2018). &amp;#32;Improving language understanding by generative pre-training.&amp;#32;Retrieved from&amp;#32; https://www.mikecaptain.com/resources/pdf/GPT-1.pdf ) architectures. For example, I can highly recommend the write-up by Turner&amp;#32;(2023 Turner,&amp;#32; R.</description>
    </item>
    
  </channel>
</rss>
