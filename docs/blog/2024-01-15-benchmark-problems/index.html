<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The benchmark problems reviving manual evaluation | arduin.io</title>
<meta name="keywords" content="">
<meta name="description" content="There is a curious trend in machine learning (ML): researchers developing the most capable large language models (LLMs) increasingly evaluate them using manual methods such as red teaming. In red teaming, researchers hire workers to manually try to break the LLM in some form by interacting with it. Similarly, some users (including myself) pick their preferred LLM assistant by manually trying out various models – checking each LLM&rsquo;s &ldquo;vibe&rdquo;. Given that LLM researchers and users both actively seek to automate all sorts of other tasks, red teaming and vibe checks are surprisingly manual evaluation processes.">
<meta name="author" content="Arduin Findeis">
<link rel="canonical" href="https://arduin.io/blog/2024-01-15-benchmark-problems/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f545874d1bf41c846127da0e7a0d159a77dede0ec07d939545d3c145dd01177d.css" integrity="sha256-9UWHTRv0HIRhJ9oOeg0Vmnfe3g7AfZOVRdPBRd0BF30=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://arduin.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://arduin.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://arduin.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://arduin.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://arduin.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script data-goatcounter="https://gbqtk.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script><meta property="og:title" content="The benchmark problems reviving manual evaluation" />
<meta property="og:description" content="There is a curious trend in machine learning (ML): researchers developing the most capable large language models (LLMs) increasingly evaluate them using manual methods such as red teaming. In red teaming, researchers hire workers to manually try to break the LLM in some form by interacting with it. Similarly, some users (including myself) pick their preferred LLM assistant by manually trying out various models – checking each LLM&rsquo;s &ldquo;vibe&rdquo;. Given that LLM researchers and users both actively seek to automate all sorts of other tasks, red teaming and vibe checks are surprisingly manual evaluation processes." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://arduin.io/blog/2024-01-15-benchmark-problems/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2024-01-15T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-01-15T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="The benchmark problems reviving manual evaluation"/>
<meta name="twitter:description" content="There is a curious trend in machine learning (ML): researchers developing the most capable large language models (LLMs) increasingly evaluate them using manual methods such as red teaming. In red teaming, researchers hire workers to manually try to break the LLM in some form by interacting with it. Similarly, some users (including myself) pick their preferred LLM assistant by manually trying out various models – checking each LLM&rsquo;s &ldquo;vibe&rdquo;. Given that LLM researchers and users both actively seek to automate all sorts of other tasks, red teaming and vibe checks are surprisingly manual evaluation processes."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blog",
      "item": "https://arduin.io/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The benchmark problems reviving manual evaluation",
      "item": "https://arduin.io/blog/2024-01-15-benchmark-problems/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The benchmark problems reviving manual evaluation",
  "name": "The benchmark problems reviving manual evaluation",
  "description": "There is a curious trend in machine learning (ML): researchers developing the most capable large language models (LLMs) increasingly evaluate them using manual methods such as red teaming. In red teaming, researchers hire workers to manually try to break the LLM in some form by interacting with it. Similarly, some users (including myself) pick their preferred LLM assistant by manually trying out various models – checking each LLM\u0026rsquo;s \u0026ldquo;vibe\u0026rdquo;. Given that LLM researchers and users both actively seek to automate all sorts of other tasks, red teaming and vibe checks are surprisingly manual evaluation processes.",
  "keywords": [
    
  ],
  "articleBody": "There is a curious trend in machine learning (ML): researchers developing the most capable large language models (LLMs) increasingly evaluate them using manual methods such as red teaming. In red teaming, researchers hire workers to manually try to break the LLM in some form by interacting with it. Similarly, some users (including myself) pick their preferred LLM assistant by manually trying out various models – checking each LLM’s “vibe”. Given that LLM researchers and users both actively seek to automate all sorts of other tasks, red teaming and vibe checks are surprisingly manual evaluation processes. This trend towards manual evaluation hints at fundamental problems that prevent more automatic evaluation methods, such as benchmarks, to be used effectively for LLMs (Ganguli, Schiefer et al., 2023 Ganguli, D., Schiefer, N., Favaro, M. \u0026 Clark, J.(2023, 10). Retrieved from https://www.anthropic.com/index/evaluating-ai-systems ; La Malfa, Petrov et al., 2023 La Malfa, E., Petrov, A., Frieder, S., Weinhuber, C., Burnell, R., Cohn, A., Shadbolt, N. \u0026 Wooldridge, M. (2023). The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges. https://doi.org/10.48550/arXiv.2309.16573 ). In this blog post, I aim to give an illustrated overview of the problems preventing LLM benchmarks from being a fully satisfactory alternative to more manual approaches.\nNote: This blog post is currently a public draft. Thoughts and feedback are very welcome at contact@arduin.io.\n1. Introduction What’s a benchmark? An ML benchmark is an automatic evaluation tool that aims to test ML models for a certain model quality. In the context of LLMs, there exist benchmarks for all sorts of model qualities: from ability to translate from French to English (Bojar, Chatterjee et al., 2015 Bojar, O., Chatterjee, R., Federmann, C., Haddow, B., Huck, M., Hokamp, C., Koehn, P., Logacheva, V., Monz, C., Negri, M., Post, M., Scarton, C., Specia, L. \u0026 Turchi, M. (2015). Findings of the 2015 Workshop on Statistical Machine Translation. Proceedings of the Tenth Workshop on Statistical Machine Translation. 1–46. Retrieved from https://www.research.ed.ac.uk/en/publications/findings-of-the-2015-workshop-on-statistical-machine-translation ) to gender bias of model output (Rudinger, Naradowsky et al., 2018 Rudinger, R., Naradowsky, J., Leonard, B. \u0026 Van Durme, B. (2018). Gender Bias in Coreference Resolution. https://doi.org/10.48550/arXiv.1804.09301 ). Exactly how a model quality is tested and measured varies widely between benchmarks – I will give a specific example that uses unit tests in the next section. I like to visualise benchmarks as “living” on top of the set of all possible model qualities. Each benchmark is shown as a flag signalling the state of the model quality it is attached to, as illustrated below.\nIllustration of a benchmark as a flag providing a signal about the underlying model quality. Each cross represents a model quality. For illustration purposes, only a handful of crosses are shown but there exist infinitely many model qualities.\nBenchmarks have been instrumental in guiding machine learning (ML) progress. Whilst inevitably imperfect, benchmarks provide an important signal as to whether one model is “better” than another. This “better” signal enables researchers to decide which ML approach to investigate further – and which to stop investigating. The sharing of benchmark results also enables the synchronisation of ML research: researchers can compare their work to others without having to reproduce their results.\nExample benchmark: HumanEval Throughout this blog post, I will use the HumanEval benchmark (Chen, Tworek et al., 2021 Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I. \u0026 Zaremba, W. (2021). Evaluating Large Language Models Trained on Code. https://doi.org/10.48550/arXiv.2107.03374 ) as an illustrative example. HumanEval aims to evaluate a model’s ability to write Python code by measuring “functional correctness for synthesizing programs from docstrings” (as the authors put it). To test the model quality, the benchmark runs 164 test cases on a model. Each test case consists of a hand-written beginning of a Python function (a function signature and docstring) and corresponding automatic code tests (unit tests). The LLM is tasked to complete the Python function based on just the beginning. The complete Python function, consisting of the pre-written beginning and the LLM’s completion, is then automatically checked using the corresponding code tests. This procedure is repeated for all 164 test cases to obtain the overall number of functions the LLM is able to successfully complete. As far as I understand, the benchmark is called HumanEval, because all its test cases were hand-written by a human – unlike other benchmarks that scrape websites like GitHub.\nExample task from HumanEval. The model is tasked to complete the Python function shown.\nDisclaimer: Whilst I use HumanEval to demonstrate problems with current benchmarks, HumanEval is a great contribution and generally no worse than most other benchmarks. I picked HumanEval because it’s a well-known and straightforward benchmark.\nManual evaluation of LLMs In ML research, it’s good practice to not just blindly rely on benchmarks (like HumanEval). Manual evaluation of model outputs is crucial to find failure cases overlooked by benchmarks. However, for LLMs, manual evaluation in the form of red teaming appears to have gained unusually high importance relative to automatic benchmarks. Instead of being the typically small sanity check as in other ML areas, manual evaluation has seemingly become an integral part of evaluating LLMs for many labs. There exist entire leaderboards (such as Chatbot Arena (Zheng, Chiang et al., 2023 Zheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. \u0026 Stoica, I. (2023). Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. https://doi.org/10.48550/arXiv.2306.05685 )) based on human judgement. Research labs developing the largest models all appear to invest heavily into manual evaluation via red teaming. Exact numbers are difficult to obtain due the secrecy of many commercial labs, but blog posts and public reporting indicate that the investement is sizable (Microsoft, 2023 Microsoft(2023, 11/6). Retrieved from https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming ; Ganguli, Schiefer et al., 2023 Ganguli, D., Schiefer, N., Favaro, M. \u0026 Clark, J.(2023, 10). Retrieved from https://www.anthropic.com/index/evaluating-ai-systems ; Ganguli, Lovitt et al., 2022 Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., Jones, A., Bowman, S., Chen, A., Conerly, T., DasSarma, N., Drain, D., Elhage, N., El-Showk, S., Fort, S., Hatfield-Dodds, Z., Henighan, T., Hernandez, D., Hume, T., Jacobson, J., Johnston, S., Kravec, S., Olsson, C., Ringer, S., Tran-Johnson, E., Amodei, D., Brown, T., Joseph, N., McCandlish, S., Olah, C., Kaplan, J. \u0026 Clark, J. (2022). Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned. https://doi.org/10.48550/arXiv.2209.07858 ; OpenAI, 2023 OpenAI(2023, 9/19). Retrieved from https://openai.com/blog/red-teaming-network ).\nLimitations of manual evaluation There is nothing wrong with doing extensive manual evaluations. On the contrary, it may be considered desirable for more humans look and verify model outputs – as opposed to models being shipped blindly relying on benchmark scores. However, just relying on manual evaluation without appropriate automatic evaluation capabilities is problematic: manual evaluation does not scale well. Firstly, extending testing to more models, model qualities, or application areas also requires more workers – making this form of scaling very costly. It may also be difficult due to limited availability of suitable experts. Secondly, there is a ceiling as to what complexity of capabilities humans can evaluate. A human worker will likely struggle to evaluate any superhuman capabilities. Thus, it is easy to conceive overreliance on manual evaluation leading to a poor understanding of increasingly capable models with diverse use-cases.\nGiven these limitations, it is crucial to identify and address the problems of automatic benchmarks – in order to augment manual evaluation with scalable automatic evaluation. As a step towards solving them, this blog post will run through what I consider to be the main problems that exist with current benchmarks.\n2. Problems with individual benchmarks Problem 1: overfitted benchmarks Often LMs are overfitted to popular benchmarks. There are two main reasons why a model may be overfitted to a specific benchmark: (1) training data contamination (Yang, Chiang et al., 2023 Yang, S., Chiang, W., Zheng, L., Gonzalez, J. \u0026 Stoica, I. (2023). Rethinking Benchmark and Contamination for Language Models with Rephrased Samples. https://doi.org/10.48550/arXiv.2311.04850 ) or (2) usage for hyperparameter tuning (Goodhart’s Law). In the first case, the benchmark’s data ends up in the model’s training data. Thus, the model may perform well on the benchmark by memorizing the correct solution, but not generalize equally well beyond the test cases. Any benchmark measurement will be distorted and mostly useless (unless you want to measure memorization). When a benchmark is used for model hyperparameter tuning, a slightly more indirect form of overfitting occurs: the benchmark data itself is not leaked but information about the data. Thus, Goodhart’s Law applies and the benchmark’s results similarly become less meaningful.\nIllustration of a benchmark that models are overfitted to. Problem 2: saturated benchmarks Usually benchmarks become saturated over time and are no longer able to effectively separate top models. When created initially, benchmarks are usually at the edge of what is possible with current models. They usually test a model quality which top models exhibit to a varying degree, and none fully exhibit. Over time, as models become more capable, the top models increasingly perform better and better on the benchmark. Eventually, there remains very little, or even no test cases, that separate different models. For example, HumanEval may be considered as quite saturated.\nIllustration of a saturated benchmark that no longer is able to distinguish between top models. Problem 3: misunderstood benchmarks Users often do not fully understand what model quality a benchmark actually evaluates. Whilst benchmarks are often built with a general model quality in mind, practical limitations mean that benchmark tasks are usually focused on a much narrower subquality. For example, HumanEval is often seen as a test for Python coding ability, yet the benchmark itself only focuses on writing individual Python functions based on docstrings. The benchmark does not cover anything like classes or more complex code structure. Additionally, as benchmarks become more saturated (i.e. best models solve most tasks), the top models get separated by a vanishingly small subset of tasks. Thus, a saturated benchmark often tests a narrower model quality than the initial quality of the benchmark. For example, in HumanEval the difference between top models like GPT-4 and Claude-2 primarily centers around a small set of problems, often with slightly ambiguous framing. In other cases, benchmarks are claimed to be more general than they actually are, leading to a misunderstanding. In practice, misunderstood benchmarks are counterproductive for decision making and can lead to reduced trust in benchmarks.\nSimplified illustration of some users misunderstand the model quality evaluated by a benchmark. For example, whilst people widely understand the HumanEval benchmark to evaluate \"coding ability\", the model quality evaluated by the benchmark may be more accurately described as \"ability to correctly complete Python functions\".\nProblem 4: expensive benchmarks Some benchmarks require significant effort to run them. There are two main reasons: (1) setup cost and (2) compute cost. The first is about how much effort it is to run the benchmark. Some popular benchmarks such as BBQ (Parrish, Chen et al., 2021 Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., Htut, P. \u0026 Bowman, S. (2021). BBQ: A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193. Retrieved from https://arxiv.org/abs/2110.08193 ) can be work intensive to get up and running. There are reports of the setup of BBQ requiring a software engineer a full work week (Ganguli, Schiefer et al., 2023 Ganguli, D., Schiefer, N., Favaro, M. \u0026 Clark, J.(2023, 10). Retrieved from https://www.anthropic.com/index/evaluating-ai-systems ). This prevents more resource-strapped users from running these benchmarks in the first place. Similarly, many benchmarks require a large amount of compute for model inference to use them. If you didn’t trust the reported results and wanted to test your OpenAI API on the standard HELM benchmark (Liang, Bommasani et al., 2022 Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C., Ré, C., Acosta-Navas, D., Hudson, D., Zelikman, E., Durmus, E., Ladhak, F., Rong, F., Ren, H., Yao, H., Wang, J., Santhanam, K., Orr, L., Zheng, L., Yuksekgonul, M., Suzgun, M., Kim, N., Guha, N., Chatterji, N., Khattab, O., Henderson, P., Huang, Q., Chi, R., Xie, S., Santurkar, S., Ganguli, S., Hashimoto, T., Icard, T., Zhang, T., Chaudhary, V., Wang, W., Li, X., Mai, Y., Zhang, Y. \u0026 Koreeda, Y. (2022). Holistic Evaluation of Language Models. https://doi.org/10.48550/arXiv.2211.09110 ), you may have to spend a lot of money (HELM lite is an interesting alternative).\nillustration of how some benchmarks are expensive to run.\n3. Problems with benchmark ecosystem Problem 5: missing benchmarks For many model qualities, benchmarks could be created but haven’t been created yet. There is an infinite number of possible model qualities to evaluate. For any given benchmark, it is easy to construct an adjacent model quality that is not yet measured by a benchmark. For example, whilst HumanEval aims to evaluate “ability to write Python code”, we may instead care about “ability to write Python code using framework X”. As benchmarks are typically handcrafted, most LM qualities do not have a corresponding existing benchmark – a benchmark for those qualities is missing. For multi-use systems like LLMs, developers are frequently exploring new use-cases with no corresponding benchmarks. The figure below illustrates how most model qualities do not have a corresponding benchmark.\nIllustration of how for many model qualities the corresponding benchmarks are missing.\nProblem 6: unfeasible benchmarks Some model qualities are too difficult to measure with current methods. Whilst HumanEval evaluates writing short Python functions, the job of a software engineer involves many other tasks, including creating pull-requests, co-ordinating with a team, or coming up with a good system architecture. Due to the complexity of testing some of these tasks, no benchmark exists that captures all model qualities required for an LM to take over an software engineering role. The field of scalable oversight (Amodei, Olah et al., 2016 Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J. \u0026 Mané, D. (2016). Concrete Problems in AI Safety. https://doi.org/10.48550/arXiv.1606.06565 ) aims to tackle the problem of evaluating increasingly complex model qualities, especially when models exhibit qualities that surpass humans capabilities.\nIllustration of how many model qualities are too complex to evaluate with current benchmark methods.\nProblem 7: aimless benchmarks Some users do not know what qualities they want their LM to have. Unlike previous ML systems, LMs have become truly multi-use. The same model may be used as writing assistant, translator, therapist, or code autocomplete engine. Due to the open-ended nature of model uses, model developers struggle to explicitly state what model qualities they aim for. Qualities tend to be very ambiguous, such as harmless and helpful. Without clarity around desirable model qualities, it is impossible to identify or create a suitable benchmark. Model developers rely heavily on red teaming as opposed to automatic benchmarks – as this more closely aligns with model use. I believe the reason why traditional benchmarks are not held in high regard by model developer, is primarily because they require the developers to decide what’s important – what qualities to test. This issue was well summarized in this podcast (Bashir, 2023 Bashir, D. (2023). 2023 in AI, with Nathan Benaich. Retrieved from https://open.spotify.com/episode/5CV9ongsBGbjtF5SgfgDNF?si=cDhbu8KET5OIB7XdsfXI6w\u0026t=1138\u0026context=spotify%3Ashow%3A6onNcSqsP6hEEqmZ6TU2g8 ). Red teaming allows model developers to implicitly let the red team members decide what model qualities to test, instead of the developers having to state them explicitly.\nIllustration of how some benchmark users aimlessly (almost randomly) select benchmarks and model qualities to evaluate. This issue often arises due to a lack of clarity around what qualities matter for their LM application. 4. Conclusion Whilst my list of problems above certainly is incomplete, I hope it helped provide some clarity around what some of the most important issues of benchmarks are. I believe that it is vital to be aware of these problems whenever using – and especially when developing – benchmarks. Each problem also presents an interesting research direction for future work.\nOverview of all problems discussed.\nWant to read further? I can recommend Anthropic’s blog post on the topic (Ganguli, Schiefer et al., 2023 Ganguli, D., Schiefer, N., Favaro, M. \u0026 Clark, J.(2023, 10). Retrieved from https://www.anthropic.com/index/evaluating-ai-systems ). If you’re looking for a more comprehensive overview over all benchmarks, consider looking at a survey over language model evaluation (e.g. Chang, Wang et al. (2023 Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P., Yang, Q. \u0026 Xie, X. (2023). A Survey on Evaluation of Large Language Models. https://doi.org/10.48550/arXiv.2307.03109 ; Guo, Jin et al. (2023 Guo, Z., Jin, R., Liu, C., Huang, Y., Shi, D., Supryadi, Yu, L., Liu, Y., Li, J., Xiong, B. \u0026 Xiong, D. (2023). Evaluating Large Language Models: A Comprehensive Survey. https://doi.org/10.48550/arXiv.2310.19736 ).)\nAcknowledgements Big thanks to the many people I have been extensively discussing benchmarks with, especially Samuel Albanie, Rob Mullins and Benjamin Minixhofer. I would also like to highlight the useful discussions I had at the Evaluating Foundation/Frontier Models Workshop organised by Emanuele La Malfa et al. – this helped clarify my thoughts.\nVersions v1 (2024-01-15): initial public draft release References Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J. \u0026 Mané, D. (2016). Concrete Problems in AI Safety. https://doi.org/10.48550/arXiv.1606.06565 Bashir, D. (2023). 2023 in AI, with Nathan Benaich. Retrieved from https://open.spotify.com/episode/5CV9ongsBGbjtF5SgfgDNF?si=cDhbu8KET5OIB7XdsfXI6w\u0026t=1138\u0026context=spotify%3Ashow%3A6onNcSqsP6hEEqmZ6TU2g8 Bojar, O., Chatterjee, R., Federmann, C., Haddow, B., Huck, M., Hokamp, C., Koehn, P., Logacheva, V., Monz, C., Negri, M., Post, M., Scarton, C., Specia, L. \u0026 Turchi, M. (2015). Findings of the 2015 Workshop on Statistical Machine Translation. Proceedings of the Tenth Workshop on Statistical Machine Translation. 1–46. Retrieved from https://www.research.ed.ac.uk/en/publications/findings-of-the-2015-workshop-on-statistical-machine-translation Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P., Yang, Q. \u0026 Xie, X. (2023). A Survey on Evaluation of Large Language Models. https://doi.org/10.48550/arXiv.2307.03109 Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I. \u0026 Zaremba, W. (2021). Evaluating Large Language Models Trained on Code. https://doi.org/10.48550/arXiv.2107.03374 Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., Jones, A., Bowman, S., Chen, A., Conerly, T., DasSarma, N., Drain, D., Elhage, N., El-Showk, S., Fort, S., Hatfield-Dodds, Z., Henighan, T., Hernandez, D., Hume, T., Jacobson, J., Johnston, S., Kravec, S., Olsson, C., Ringer, S., Tran-Johnson, E., Amodei, D., Brown, T., Joseph, N., McCandlish, S., Olah, C., Kaplan, J. \u0026 Clark, J. (2022). Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned. https://doi.org/10.48550/arXiv.2209.07858 Ganguli, D., Schiefer, N., Favaro, M. \u0026 Clark, J.(2023, 10). Retrieved from https://www.anthropic.com/index/evaluating-ai-systems Guo, Z., Jin, R., Liu, C., Huang, Y., Shi, D., Supryadi, Yu, L., Liu, Y., Li, J., Xiong, B. \u0026 Xiong, D. (2023). Evaluating Large Language Models: A Comprehensive Survey. https://doi.org/10.48550/arXiv.2310.19736 La Malfa, E., Petrov, A., Frieder, S., Weinhuber, C., Burnell, R., Cohn, A., Shadbolt, N. \u0026 Wooldridge, M. (2023). The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges. https://doi.org/10.48550/arXiv.2309.16573 Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C., Ré, C., Acosta-Navas, D., Hudson, D., Zelikman, E., Durmus, E., Ladhak, F., Rong, F., Ren, H., Yao, H., Wang, J., Santhanam, K., Orr, L., Zheng, L., Yuksekgonul, M., Suzgun, M., Kim, N., Guha, N., Chatterji, N., Khattab, O., Henderson, P., Huang, Q., Chi, R., Xie, S., Santurkar, S., Ganguli, S., Hashimoto, T., Icard, T., Zhang, T., Chaudhary, V., Wang, W., Li, X., Mai, Y., Zhang, Y. \u0026 Koreeda, Y. (2022). Holistic Evaluation of Language Models. https://doi.org/10.48550/arXiv.2211.09110 Microsoft(2023, 11/6). Retrieved from https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming OpenAI(2023, 9/19). Retrieved from https://openai.com/blog/red-teaming-network Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., Htut, P. \u0026 Bowman, S. (2021). BBQ: A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193. Retrieved from https://arxiv.org/abs/2110.08193 Rudinger, R., Naradowsky, J., Leonard, B. \u0026 Van Durme, B. (2018). Gender Bias in Coreference Resolution. https://doi.org/10.48550/arXiv.1804.09301 Yang, S., Chiang, W., Zheng, L., Gonzalez, J. \u0026 Stoica, I. (2023). Rethinking Benchmark and Contamination for Language Models with Rephrased Samples. https://doi.org/10.48550/arXiv.2311.04850 Zheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. \u0026 Stoica, I. (2023). Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. https://doi.org/10.48550/arXiv.2306.05685 Appendix A. Additional problems The previous list is not necessarily exhaustive. Some problems related to benchmarks are not about benchmarks themselves, but the systems we are trying to evaluate. Other problems may be considered sub-problems of one of the mentioned problems.\nModel stochasticity: difficult to reproduce when using models via LLM-as-a-Service Brittle benchmarks that can be easily fooled and are not robust to minor prompt pertubations. May be covered by misunderstood and overfitted cases. (Red teaming lacks standardisation: not about benchmarks, but a common complaint in the community) ",
  "wordCount" : "3613",
  "inLanguage": "en",
  "datePublished": "2024-01-15T00:00:00Z",
  "dateModified": "2024-01-15T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Arduin Findeis"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://arduin.io/blog/2024-01-15-benchmark-problems/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "arduin.io",
    "logo": {
      "@type": "ImageObject",
      "url": "https://arduin.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://arduin.io/" accesskey="h" title="arduin.io (Alt + H)">arduin.io</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://arduin.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://arduin.io/research" title="Research">
                    <span>Research</span>
                </a>
            </li>
            <li>
                <a href="https://arduin.io/projects" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://arduin.io/bio" title="Bio">
                    <span>Bio</span>
                </a>
            </li>
            <li>
                <a href="https://arduin.io/blog" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://arduin.io/newsletter" title="Newsletter">
                    <span>Newsletter</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <link rel="stylesheet" type="text/css" href="/hugo-cite.css" />
    <h1 class="post-title">
      The benchmark problems reviving manual evaluation
    </h1>
    <div class="post-meta"><span title='2024-01-15 00:00:00 +0000 UTC'>January 15, 2024</span>&nbsp;·&nbsp;17 min&nbsp;·&nbsp;Arduin Findeis

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-introduction" aria-label="1. Introduction">1. Introduction</a><ul>
                        
                <li>
                    <a href="#whats-a-benchmark" aria-label="What&amp;rsquo;s a benchmark?">What&rsquo;s a benchmark?</a></li>
                <li>
                    <a href="#example-benchmark-humaneval" aria-label="Example benchmark: HumanEval">Example benchmark: HumanEval</a></li>
                <li>
                    <a href="#manual-evaluation-of-llms" aria-label="Manual evaluation of LLMs">Manual evaluation of LLMs</a></li>
                <li>
                    <a href="#limitations-of-manual-evaluation" aria-label="Limitations of manual evaluation">Limitations of manual evaluation</a></li></ul>
                </li>
                <li>
                    <a href="#2-problems-with-individual-benchmarks" aria-label="2. Problems with individual benchmarks">2. Problems with individual benchmarks</a><ul>
                        
                <li>
                    <a href="#problem-1-overfitted-benchmarks" aria-label="Problem 1: overfitted benchmarks">Problem 1: <em>overfitted</em> benchmarks</a></li>
                <li>
                    <a href="#problem-2-saturated-benchmarks" aria-label="Problem 2: saturated benchmarks">Problem 2: <em>saturated</em> benchmarks</a></li>
                <li>
                    <a href="#problem-3-misunderstood-benchmarks" aria-label="Problem 3: misunderstood benchmarks">Problem 3: <em>misunderstood</em> benchmarks</a></li>
                <li>
                    <a href="#problem-4-expensive-benchmarks" aria-label="Problem 4: expensive benchmarks">Problem 4: <em>expensive</em> benchmarks</a></li></ul>
                </li>
                <li>
                    <a href="#3-problems-with-benchmark-ecosystem" aria-label="3. Problems with benchmark ecosystem">3. Problems with benchmark ecosystem</a><ul>
                        
                <li>
                    <a href="#problem-5-missing-benchmarks" aria-label="Problem 5: missing benchmarks">Problem 5: <em>missing</em> benchmarks</a></li>
                <li>
                    <a href="#problem-6-unfeasible-benchmarks" aria-label="Problem 6: unfeasible benchmarks">Problem 6: <em>unfeasible</em> benchmarks</a></li>
                <li>
                    <a href="#problem-7-aimless-benchmarks" aria-label="Problem 7: aimless benchmarks">Problem 7: <em>aimless</em> benchmarks</a></li></ul>
                </li>
                <li>
                    <a href="#4-conclusion" aria-label="4. Conclusion">4. Conclusion</a></li>
                <li>
                    <a href="#acknowledgements" aria-label="Acknowledgements">Acknowledgements</a></li>
                <li>
                    <a href="#versions" aria-label="Versions">Versions</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a></li>
                <li>
                    <a href="#appendix" aria-label="Appendix">Appendix</a><ul>
                        
                <li>
                    <a href="#a-additional-problems" aria-label="A. Additional problems">A. Additional problems</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>There is a curious trend in <em>machine learning</em> (ML): researchers developing the most capable <em>large language models</em> (LLMs) increasingly evaluate them using manual methods such as <em>red teaming</em>. In red teaming, researchers hire workers to manually try to break the LLM in some form by interacting with it. Similarly, some users (including myself) pick their preferred LLM assistant by manually trying out various models – checking each LLM&rsquo;s <em>&ldquo;vibe&rdquo;</em>. Given that LLM researchers and users both actively seek to automate all sorts of other tasks, red teaming and vibe checks are <em>surprisingly manual evaluation processes</em>. This trend towards manual evaluation hints at fundamental problems that prevent more automatic evaluation methods, such as benchmarks, to be used effectively for LLMs 
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#ganguli2023challengesevaluatingai"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Deep"><span itemprop="familyName">Ganguli</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Nicholas"><span itemprop="familyName">Schiefer</span></span>
                et al.,&#32;<span itemprop="datePublished">2023</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/WebPage"
      data-type="webpage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ganguli</span>,&#32;
    <meta itemprop="givenName" content="Deep" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Schiefer</span>,&#32;
    <meta itemprop="givenName" content="Nicholas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Favaro</span>,&#32;
    <meta itemprop="givenName" content="Marina" />
    M.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Clark</span>,&#32;
    <meta itemprop="givenName" content="Jack" />
    J.</span>(<span itemprop="datePublished">2023,&#32;10</span>).&#32;Retrieved from&#32;
  <a href="https://www.anthropic.com/index/evaluating-ai-systems"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://www.anthropic.com/index/evaluating-ai-systems</a></span>




</span></span>;&#32;<span class="hugo-cite-group"><a href="#lamalfa2023arrtlanguagemodelsasaserviceoverview"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Emanuele"><span itemprop="familyName">La Malfa</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Aleksandar"><span itemprop="familyName">Petrov</span></span>
                et al.,&#32;<span itemprop="datePublished">2023</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">La Malfa</span>,&#32;
    <meta itemprop="givenName" content="Emanuele" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Petrov</span>,&#32;
    <meta itemprop="givenName" content="Aleksandar" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Frieder</span>,&#32;
    <meta itemprop="givenName" content="Simon" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Weinhuber</span>,&#32;
    <meta itemprop="givenName" content="Christoph" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Burnell</span>,&#32;
    <meta itemprop="givenName" content="Ryan" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cohn</span>,&#32;
    <meta itemprop="givenName" content="Anthony G." />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Shadbolt</span>,&#32;
    <meta itemprop="givenName" content="Nigel" />
    N.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wooldridge</span>,&#32;
    <meta itemprop="givenName" content="Michael" />
    M.</span>
  &#32;
    (<span itemprop="datePublished">2023</span>).
  &#32;<span itemprop="name">The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges</span>.
  <a href="https://doi.org/10.48550/arXiv.2309.16573"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2309.16573</a></span>




</span></span>)</span>. In this blog post, I aim to give an illustrated overview of the problems preventing LLM benchmarks from being a fully satisfactory alternative to more manual approaches.</p>
<blockquote>
<p><strong>Note:</strong> This blog post is currently a <em>public draft</em>. Thoughts and feedback are very welcome at <a href="mailto:contact@arduin.io">contact@arduin.io</a>.</p>
</blockquote>
<hr>
<h2 id="1-introduction">1. Introduction<a hidden class="anchor" aria-hidden="true" href="#1-introduction">#</a></h2>
<h3 id="whats-a-benchmark">What&rsquo;s a benchmark?<a hidden class="anchor" aria-hidden="true" href="#whats-a-benchmark">#</a></h3>
<p>An ML benchmark is an automatic evaluation tool that aims to test ML models for a certain <em>model quality</em>. In the context of LLMs, there exist benchmarks for all sorts of model qualities: from <em>ability to translate from French to English</em> 
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#bojar2015findings2015workshop"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Ondrej"><span itemprop="familyName">Bojar</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Rajen"><span itemprop="familyName">Chatterjee</span></span>
                et al.,&#32;<span itemprop="datePublished">2015</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bojar</span>,&#32;
    <meta itemprop="givenName" content="Ondrej" />
    O.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chatterjee</span>,&#32;
    <meta itemprop="givenName" content="Rajen" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Federmann</span>,&#32;
    <meta itemprop="givenName" content="Christian" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Haddow</span>,&#32;
    <meta itemprop="givenName" content="Barry" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Huck</span>,&#32;
    <meta itemprop="givenName" content="Matthias" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hokamp</span>,&#32;
    <meta itemprop="givenName" content="Chris" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Koehn</span>,&#32;
    <meta itemprop="givenName" content="Philipp" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Logacheva</span>,&#32;
    <meta itemprop="givenName" content="Varvara" />
    V.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Monz</span>,&#32;
    <meta itemprop="givenName" content="Christof" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Negri</span>,&#32;
    <meta itemprop="givenName" content="Matteo" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Post</span>,&#32;
    <meta itemprop="givenName" content="Matt" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Scarton</span>,&#32;
    <meta itemprop="givenName" content="Carolina" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Specia</span>,&#32;
    <meta itemprop="givenName" content="Lucia" />
    L.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Turchi</span>,&#32;
    <meta itemprop="givenName" content="Marco" />
    M.</span>
  &#32;
    (<span itemprop="datePublished">2015</span>).
  &#32;<span itemprop="name">Findings of the 2015 Workshop on Statistical Machine Translation</span>.<i>
    <span itemprop="about">Proceedings of the Tenth Workshop on Statistical Machine Translation</span></i>.&#32;<span itemprop="pagination">1–46</span>.&#32;Retrieved from&#32;
  <a href="https://www.research.ed.ac.uk/en/publications/findings-of-the-2015-workshop-on-statistical-machine-translation"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://www.research.ed.ac.uk/en/publications/findings-of-the-2015-workshop-on-statistical-machine-translation</a></span>




</span></span>)</span> to <em>gender bias of model output</em> 
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#rudinger2018genderbiascoreference"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Rachel"><span itemprop="familyName">Rudinger</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Jason"><span itemprop="familyName">Naradowsky</span></span>
                et al.,&#32;<span itemprop="datePublished">2018</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Rudinger</span>,&#32;
    <meta itemprop="givenName" content="Rachel" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Naradowsky</span>,&#32;
    <meta itemprop="givenName" content="Jason" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Leonard</span>,&#32;
    <meta itemprop="givenName" content="Brian" />
    B.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Van Durme</span>,&#32;
    <meta itemprop="givenName" content="Benjamin" />
    B.</span>
  &#32;
    (<span itemprop="datePublished">2018</span>).
  &#32;<span itemprop="name">Gender Bias in Coreference Resolution</span>.
  <a href="https://doi.org/10.48550/arXiv.1804.09301"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.1804.09301</a></span>




</span></span>)</span>. Exactly <em>how</em> a model quality is tested and measured varies widely between benchmarks – I will give a specific example that uses unit tests in the next section. I like to visualise benchmarks as &ldquo;living&rdquo; on top of the set of all possible model qualities. Each benchmark is shown as a flag <em>signalling</em> the state of the model quality it is attached to, as illustrated below.</p>



<figure style="text-align: center;"><img
         src="img/00_info_v2.png" width="45%"style="display:inline"
    /><figcaption>
        <p align="center">Illustration of a benchmark as a flag providing a signal about the underlying model quality. Each cross represents a model quality. For illustration purposes, only a handful of crosses are shown but there exist infinitely many model qualities.</p>
    </figcaption>
</figure>
<p>Benchmarks have been instrumental in guiding machine learning (ML) progress. Whilst inevitably imperfect, benchmarks provide an important signal as to whether one model is <em>&ldquo;better&rdquo;</em> than another. This &ldquo;better&rdquo; signal enables researchers to decide which ML approach to investigate further – and which to stop investigating. The sharing of benchmark results also enables the <em>synchronisation</em> of ML research: researchers can compare their work to others without having to reproduce their results.</p>
<h3 id="example-benchmark-humaneval">Example benchmark: HumanEval<a hidden class="anchor" aria-hidden="true" href="#example-benchmark-humaneval">#</a></h3>
<p>Throughout this blog post, I will use the <em>HumanEval</em> benchmark 
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#chen2021evaluatinglargelanguage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Mark"><span itemprop="familyName">Chen</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Jerry"><span itemprop="familyName">Tworek</span></span>
                et al.,&#32;<span itemprop="datePublished">2021</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chen</span>,&#32;
    <meta itemprop="givenName" content="Mark" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Tworek</span>,&#32;
    <meta itemprop="givenName" content="Jerry" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Jun</span>,&#32;
    <meta itemprop="givenName" content="Heewoo" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yuan</span>,&#32;
    <meta itemprop="givenName" content="Qiming" />
    Q.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Pinto</span>,&#32;
    <meta itemprop="givenName" content="Henrique Ponde de Oliveira" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kaplan</span>,&#32;
    <meta itemprop="givenName" content="Jared" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Edwards</span>,&#32;
    <meta itemprop="givenName" content="Harri" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Burda</span>,&#32;
    <meta itemprop="givenName" content="Yuri" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Joseph</span>,&#32;
    <meta itemprop="givenName" content="Nicholas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Brockman</span>,&#32;
    <meta itemprop="givenName" content="Greg" />
    G.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ray</span>,&#32;
    <meta itemprop="givenName" content="Alex" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Puri</span>,&#32;
    <meta itemprop="givenName" content="Raul" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Krueger</span>,&#32;
    <meta itemprop="givenName" content="Gretchen" />
    G.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Petrov</span>,&#32;
    <meta itemprop="givenName" content="Michael" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Khlaaf</span>,&#32;
    <meta itemprop="givenName" content="Heidy" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sastry</span>,&#32;
    <meta itemprop="givenName" content="Girish" />
    G.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mishkin</span>,&#32;
    <meta itemprop="givenName" content="Pamela" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chan</span>,&#32;
    <meta itemprop="givenName" content="Brooke" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gray</span>,&#32;
    <meta itemprop="givenName" content="Scott" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ryder</span>,&#32;
    <meta itemprop="givenName" content="Nick" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Pavlov</span>,&#32;
    <meta itemprop="givenName" content="Mikhail" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Power</span>,&#32;
    <meta itemprop="givenName" content="Alethea" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kaiser</span>,&#32;
    <meta itemprop="givenName" content="Lukasz" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bavarian</span>,&#32;
    <meta itemprop="givenName" content="Mohammad" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Winter</span>,&#32;
    <meta itemprop="givenName" content="Clemens" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Tillet</span>,&#32;
    <meta itemprop="givenName" content="Philippe" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Such</span>,&#32;
    <meta itemprop="givenName" content="Felipe Petroski" />
    F.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cummings</span>,&#32;
    <meta itemprop="givenName" content="Dave" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Plappert</span>,&#32;
    <meta itemprop="givenName" content="Matthias" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chantzis</span>,&#32;
    <meta itemprop="givenName" content="Fotios" />
    F.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Barnes</span>,&#32;
    <meta itemprop="givenName" content="Elizabeth" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Herbert-Voss</span>,&#32;
    <meta itemprop="givenName" content="Ariel" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Guss</span>,&#32;
    <meta itemprop="givenName" content="William Hebgen" />
    W.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Nichol</span>,&#32;
    <meta itemprop="givenName" content="Alex" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Paino</span>,&#32;
    <meta itemprop="givenName" content="Alex" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Tezak</span>,&#32;
    <meta itemprop="givenName" content="Nikolas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Tang</span>,&#32;
    <meta itemprop="givenName" content="Jie" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Babuschkin</span>,&#32;
    <meta itemprop="givenName" content="Igor" />
    I.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Balaji</span>,&#32;
    <meta itemprop="givenName" content="Suchir" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Jain</span>,&#32;
    <meta itemprop="givenName" content="Shantanu" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Saunders</span>,&#32;
    <meta itemprop="givenName" content="William" />
    W.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hesse</span>,&#32;
    <meta itemprop="givenName" content="Christopher" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Carr</span>,&#32;
    <meta itemprop="givenName" content="Andrew N." />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Leike</span>,&#32;
    <meta itemprop="givenName" content="Jan" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Achiam</span>,&#32;
    <meta itemprop="givenName" content="Josh" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Misra</span>,&#32;
    <meta itemprop="givenName" content="Vedant" />
    V.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Morikawa</span>,&#32;
    <meta itemprop="givenName" content="Evan" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Radford</span>,&#32;
    <meta itemprop="givenName" content="Alec" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Knight</span>,&#32;
    <meta itemprop="givenName" content="Matthew" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Brundage</span>,&#32;
    <meta itemprop="givenName" content="Miles" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Murati</span>,&#32;
    <meta itemprop="givenName" content="Mira" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mayer</span>,&#32;
    <meta itemprop="givenName" content="Katie" />
    K.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Welinder</span>,&#32;
    <meta itemprop="givenName" content="Peter" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">McGrew</span>,&#32;
    <meta itemprop="givenName" content="Bob" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Amodei</span>,&#32;
    <meta itemprop="givenName" content="Dario" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">McCandlish</span>,&#32;
    <meta itemprop="givenName" content="Sam" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sutskever</span>,&#32;
    <meta itemprop="givenName" content="Ilya" />
    I.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zaremba</span>,&#32;
    <meta itemprop="givenName" content="Wojciech" />
    W.</span>
  &#32;
    (<span itemprop="datePublished">2021</span>).
  &#32;<span itemprop="name">Evaluating Large Language Models Trained on Code</span>.
  <a href="https://doi.org/10.48550/arXiv.2107.03374"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2107.03374</a></span>




</span></span>)</span> as an illustrative example. HumanEval aims to evaluate a model&rsquo;s <em>ability to write Python code</em> by measuring <em>&ldquo;functional correctness for synthesizing programs from docstrings&rdquo;</em> (as the authors put it). To test the model quality, the benchmark runs 164 <em>test cases</em> on a model. Each test case consists of a hand-written beginning of a Python function (a <em>function signature</em> and <em>docstring</em>) and corresponding automatic code tests (<em>unit tests</em>). The LLM is tasked to complete the Python function based on just the beginning. The complete Python function, consisting of the pre-written beginning and the LLM&rsquo;s completion, is then automatically checked using the corresponding code tests. This procedure is repeated for all 164 test cases to obtain the overall number of functions the LLM is able to successfully complete. As far as I understand, the benchmark is called <em>Human</em>Eval, because all its test cases were hand-written by a <em>human</em> – unlike other benchmarks that scrape websites like GitHub.</p>



<figure style="text-align: center;"><img
         src="img/00_humaneval_v0.png" width="50%"style="display:inline"
    /><figcaption>
        <p align="center">Example task from HumanEval. The model is tasked to complete the Python function shown.</p>
    </figcaption>
</figure>
<blockquote>
<p><em><strong>Disclaimer:</strong> Whilst I use HumanEval to demonstrate problems with current benchmarks, HumanEval is a great contribution and generally no worse than most other benchmarks. I picked HumanEval because it&rsquo;s a well-known and straightforward benchmark.</em></p>
</blockquote>
<h3 id="manual-evaluation-of-llms">Manual evaluation of LLMs<a hidden class="anchor" aria-hidden="true" href="#manual-evaluation-of-llms">#</a></h3>
<p>In ML research, it&rsquo;s good practice to not just blindly rely on benchmarks (like HumanEval). Manual evaluation of model outputs is crucial to find failure cases overlooked by benchmarks. However, for LLMs, manual evaluation in the form of red teaming appears to have gained unusually high importance relative to automatic benchmarks. Instead of being the typically small sanity check as in other ML areas, manual evaluation has seemingly become an integral part of evaluating LLMs for many labs. There exist entire leaderboards (such as <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">Chatbot Arena</a> 
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#zheng2023judgingllmasajudgemtbench"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Lianmin"><span itemprop="familyName">Zheng</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Wei-Lin"><span itemprop="familyName">Chiang</span></span>
                et al.,&#32;<span itemprop="datePublished">2023</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zheng</span>,&#32;
    <meta itemprop="givenName" content="Lianmin" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chiang</span>,&#32;
    <meta itemprop="givenName" content="Wei-Lin" />
    W.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sheng</span>,&#32;
    <meta itemprop="givenName" content="Ying" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhuang</span>,&#32;
    <meta itemprop="givenName" content="Siyuan" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wu</span>,&#32;
    <meta itemprop="givenName" content="Zhanghao" />
    Z.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhuang</span>,&#32;
    <meta itemprop="givenName" content="Yonghao" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Lin</span>,&#32;
    <meta itemprop="givenName" content="Zi" />
    Z.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
    <meta itemprop="givenName" content="Zhuohan" />
    Z.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
    <meta itemprop="givenName" content="Dacheng" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xing</span>,&#32;
    <meta itemprop="givenName" content="Eric P." />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
    <meta itemprop="givenName" content="Hao" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gonzalez</span>,&#32;
    <meta itemprop="givenName" content="Joseph E." />
    J.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Stoica</span>,&#32;
    <meta itemprop="givenName" content="Ion" />
    I.</span>
  &#32;
    (<span itemprop="datePublished">2023</span>).
  &#32;<span itemprop="name">Judging LLM-as-a-judge with MT-Bench and Chatbot Arena</span>.
  <a href="https://doi.org/10.48550/arXiv.2306.05685"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2306.05685</a></span>




</span></span>)</span>) based on human judgement. Research labs developing the largest models all appear to invest heavily into manual evaluation via red teaming. Exact numbers are difficult to obtain due the secrecy of many commercial labs, but blog posts and public reporting indicate that the investement is sizable 
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#microsoft2023planningredteaming"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Microsoft</span></span>,&#32;<span itemprop="datePublished">2023</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/WebPage"
      data-type="webpage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Microsoft</span></span>(<span itemprop="datePublished">2023,&#32;11/6</span>).&#32;Retrieved from&#32;
  <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming</a></span>




</span></span>;&#32;<span class="hugo-cite-group"><a href="#ganguli2023challengesevaluatingai"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Deep"><span itemprop="familyName">Ganguli</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Nicholas"><span itemprop="familyName">Schiefer</span></span>
                et al.,&#32;<span itemprop="datePublished">2023</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/WebPage"
      data-type="webpage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ganguli</span>,&#32;
    <meta itemprop="givenName" content="Deep" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Schiefer</span>,&#32;
    <meta itemprop="givenName" content="Nicholas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Favaro</span>,&#32;
    <meta itemprop="givenName" content="Marina" />
    M.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Clark</span>,&#32;
    <meta itemprop="givenName" content="Jack" />
    J.</span>(<span itemprop="datePublished">2023,&#32;10</span>).&#32;Retrieved from&#32;
  <a href="https://www.anthropic.com/index/evaluating-ai-systems"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://www.anthropic.com/index/evaluating-ai-systems</a></span>




</span></span>;&#32;<span class="hugo-cite-group"><a href="#ganguli2022redteaminglanguage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Deep"><span itemprop="familyName">Ganguli</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Liane"><span itemprop="familyName">Lovitt</span></span>
                et al.,&#32;<span itemprop="datePublished">2022</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ganguli</span>,&#32;
    <meta itemprop="givenName" content="Deep" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Lovitt</span>,&#32;
    <meta itemprop="givenName" content="Liane" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kernion</span>,&#32;
    <meta itemprop="givenName" content="Jackson" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Askell</span>,&#32;
    <meta itemprop="givenName" content="Amanda" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bai</span>,&#32;
    <meta itemprop="givenName" content="Yuntao" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kadavath</span>,&#32;
    <meta itemprop="givenName" content="Saurav" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mann</span>,&#32;
    <meta itemprop="givenName" content="Ben" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Perez</span>,&#32;
    <meta itemprop="givenName" content="Ethan" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Schiefer</span>,&#32;
    <meta itemprop="givenName" content="Nicholas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ndousse</span>,&#32;
    <meta itemprop="givenName" content="Kamal" />
    K.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Jones</span>,&#32;
    <meta itemprop="givenName" content="Andy" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bowman</span>,&#32;
    <meta itemprop="givenName" content="Sam" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chen</span>,&#32;
    <meta itemprop="givenName" content="Anna" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Conerly</span>,&#32;
    <meta itemprop="givenName" content="Tom" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">DasSarma</span>,&#32;
    <meta itemprop="givenName" content="Nova" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Drain</span>,&#32;
    <meta itemprop="givenName" content="Dawn" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Elhage</span>,&#32;
    <meta itemprop="givenName" content="Nelson" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">El-Showk</span>,&#32;
    <meta itemprop="givenName" content="Sheer" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Fort</span>,&#32;
    <meta itemprop="givenName" content="Stanislav" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hatfield-Dodds</span>,&#32;
    <meta itemprop="givenName" content="Zac" />
    Z.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Henighan</span>,&#32;
    <meta itemprop="givenName" content="Tom" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hernandez</span>,&#32;
    <meta itemprop="givenName" content="Danny" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hume</span>,&#32;
    <meta itemprop="givenName" content="Tristan" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Jacobson</span>,&#32;
    <meta itemprop="givenName" content="Josh" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Johnston</span>,&#32;
    <meta itemprop="givenName" content="Scott" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kravec</span>,&#32;
    <meta itemprop="givenName" content="Shauna" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Olsson</span>,&#32;
    <meta itemprop="givenName" content="Catherine" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ringer</span>,&#32;
    <meta itemprop="givenName" content="Sam" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Tran-Johnson</span>,&#32;
    <meta itemprop="givenName" content="Eli" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Amodei</span>,&#32;
    <meta itemprop="givenName" content="Dario" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Brown</span>,&#32;
    <meta itemprop="givenName" content="Tom" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Joseph</span>,&#32;
    <meta itemprop="givenName" content="Nicholas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">McCandlish</span>,&#32;
    <meta itemprop="givenName" content="Sam" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Olah</span>,&#32;
    <meta itemprop="givenName" content="Chris" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kaplan</span>,&#32;
    <meta itemprop="givenName" content="Jared" />
    J.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Clark</span>,&#32;
    <meta itemprop="givenName" content="Jack" />
    J.</span>
  &#32;
    (<span itemprop="datePublished">2022</span>).
  &#32;<span itemprop="name">Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned</span>.
  <a href="https://doi.org/10.48550/arXiv.2209.07858"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2209.07858</a></span>




</span></span>;&#32;<span class="hugo-cite-group"><a href="#openai2023openairedteaming"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">OpenAI</span></span>,&#32;<span itemprop="datePublished">2023</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/WebPage"
      data-type="webpage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">OpenAI</span></span>(<span itemprop="datePublished">2023,&#32;9/19</span>).&#32;Retrieved from&#32;
  <a href="https://openai.com/blog/red-teaming-network"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://openai.com/blog/red-teaming-network</a></span>




</span></span>)</span>.</p>
<h3 id="limitations-of-manual-evaluation">Limitations of manual evaluation<a hidden class="anchor" aria-hidden="true" href="#limitations-of-manual-evaluation">#</a></h3>
<p>There is nothing wrong with doing extensive manual evaluations. On the contrary, it may be considered desirable for more humans look and verify model outputs – as opposed to models being shipped blindly relying on benchmark scores. However, <em>just</em> relying on manual evaluation without appropriate automatic evaluation capabilities is problematic: manual evaluation does not scale well. Firstly, extending testing to more <em>models</em>, <em>model qualities</em>, or <em>application areas</em> also requires <em>more</em> workers – making this form of scaling very costly. It may also be difficult due to limited availability of suitable experts. Secondly, there is a ceiling as to what complexity of capabilities humans can evaluate. A human worker will likely struggle to evaluate any superhuman capabilities. Thus, it is easy to conceive overreliance on manual evaluation leading to a poor understanding of increasingly capable models with diverse use-cases.</p>
<p>Given these limitations, it is crucial to identify and address the problems of automatic benchmarks – in order to augment manual evaluation with scalable automatic evaluation. As a step towards solving them, this blog post will run through what I consider to be the main problems that exist with current benchmarks.</p>
<hr>
<h2 id="2-problems-with-individual-benchmarks">2. Problems with individual benchmarks<a hidden class="anchor" aria-hidden="true" href="#2-problems-with-individual-benchmarks">#</a></h2>
<h3 id="problem-1-overfitted-benchmarks">Problem 1: <em>overfitted</em> benchmarks<a hidden class="anchor" aria-hidden="true" href="#problem-1-overfitted-benchmarks">#</a></h3>
<p><em>Often LMs are overfitted to popular benchmarks.</em> There are two main reasons why a model may be overfitted to a specific benchmark: (1) <em>training data contamination</em>  
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#yang2023rethinkingbenchmarkcontamination"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Shuo"><span itemprop="familyName">Yang</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Wei-Lin"><span itemprop="familyName">Chiang</span></span>
                et al.,&#32;<span itemprop="datePublished">2023</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yang</span>,&#32;
    <meta itemprop="givenName" content="Shuo" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chiang</span>,&#32;
    <meta itemprop="givenName" content="Wei-Lin" />
    W.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zheng</span>,&#32;
    <meta itemprop="givenName" content="Lianmin" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gonzalez</span>,&#32;
    <meta itemprop="givenName" content="Joseph E." />
    J.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Stoica</span>,&#32;
    <meta itemprop="givenName" content="Ion" />
    I.</span>
  &#32;
    (<span itemprop="datePublished">2023</span>).
  &#32;<span itemprop="name">Rethinking Benchmark and Contamination for Language Models with Rephrased Samples</span>.
  <a href="https://doi.org/10.48550/arXiv.2311.04850"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2311.04850</a></span>




</span></span>)</span> or (2) <em>usage for hyperparameter tuning</em> (<a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart&rsquo;s Law</a>). In the first case, the benchmark&rsquo;s data ends up in the model&rsquo;s training data. Thus, the model may perform well on the benchmark by <em>memorizing</em> the correct solution, but not generalize equally well beyond the test cases. Any benchmark measurement will be distorted and <em>mostly useless</em> (unless you want to measure memorization). When a benchmark is used for model hyperparameter tuning, a slightly more indirect form of overfitting occurs: the benchmark data itself is not leaked but information about the data. Thus, <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart&rsquo;s Law</a> applies and the benchmark&rsquo;s results similarly become less meaningful.</p>



<figure style="text-align: center;"><img
         src="img/03_overfitted_v2.png" width="42%"style="display:inline"
    /><figcaption>
        <p align="center">Illustration of a benchmark that models are overfitted to. </p>
    </figcaption>
</figure>
<h3 id="problem-2-saturated-benchmarks">Problem 2: <em>saturated</em> benchmarks<a hidden class="anchor" aria-hidden="true" href="#problem-2-saturated-benchmarks">#</a></h3>
<p><em>Usually benchmarks become saturated over time and are no longer able to effectively separate top models.</em> When created initially, benchmarks are usually at the edge of what is possible with current models. They usually test a model quality which top models exhibit to a <em>varying</em> degree, and none fully exhibit. Over time, as models become more capable, the top models increasingly perform better and better on the benchmark. Eventually, there remains very little, or even no test cases, that separate different models. For example, HumanEval may be considered as quite saturated.</p>



<figure style="text-align: center;"><img
         src="img/07_saturated_v0.png" width="55%"style="display:inline"
    /><figcaption>
        <p align="center">Illustration of a saturated benchmark that no longer is able to distinguish between top models. </p>
    </figcaption>
</figure>
<h3 id="problem-3-misunderstood-benchmarks">Problem 3: <em>misunderstood</em> benchmarks<a hidden class="anchor" aria-hidden="true" href="#problem-3-misunderstood-benchmarks">#</a></h3>
<p><em>Users often do not fully understand what model quality a benchmark actually evaluates.</em> Whilst benchmarks are often built with a general model quality in mind, practical limitations mean that benchmark tasks are usually focused on a much narrower subquality. For example, HumanEval is often seen as a test for <em>Python coding ability</em>, yet the benchmark itself only focuses on writing individual Python functions based on docstrings. The benchmark does not cover anything like classes or more complex code structure. Additionally, as benchmarks become more saturated (i.e. best models solve most tasks), the top models get separated by a vanishingly small subset of tasks. Thus, a saturated benchmark often tests a narrower model quality than the initial quality of the benchmark. For example, in HumanEval the difference between top models like GPT-4 and Claude-2 primarily centers around a small set of problems, often with slightly ambiguous framing. In other cases, benchmarks are claimed to be more general than they actually are, leading to a misunderstanding. In practice, <em>misunderstood</em> benchmarks are counterproductive for decision making and can lead to reduced trust in benchmarks.</p>



<figure style="text-align: center;"><img
         src="img/05_misunderstood_v1.png" width="90%"style="display:inline"
    /><figcaption>
        <p align="center">Simplified illustration of some users misunderstand the model quality evaluated by a benchmark. For example, whilst people widely understand the HumanEval benchmark to evaluate "coding ability", the model quality evaluated by the benchmark may be more accurately described as "ability to correctly complete Python functions".</p>
    </figcaption>
</figure>
<h3 id="problem-4-expensive-benchmarks">Problem 4: <em>expensive</em> benchmarks<a hidden class="anchor" aria-hidden="true" href="#problem-4-expensive-benchmarks">#</a></h3>
<p><em>Some benchmarks require significant effort to run them.</em> There are two main reasons: (1) <em>setup cost</em> and (2) <em>compute cost</em>. The first is about how much effort it is to run the benchmark. Some popular benchmarks such as BBQ 
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#parrish2021bbqhandbuiltbias"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Alicia"><span itemprop="familyName">Parrish</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Angelica"><span itemprop="familyName">Chen</span></span>
                et al.,&#32;<span itemprop="datePublished">2021</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Parrish</span>,&#32;
    <meta itemprop="givenName" content="Alicia" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chen</span>,&#32;
    <meta itemprop="givenName" content="Angelica" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Nangia</span>,&#32;
    <meta itemprop="givenName" content="Nikita" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Padmakumar</span>,&#32;
    <meta itemprop="givenName" content="Vishakh" />
    V.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Phang</span>,&#32;
    <meta itemprop="givenName" content="Jason" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Thompson</span>,&#32;
    <meta itemprop="givenName" content="Jana" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Htut</span>,&#32;
    <meta itemprop="givenName" content="Phu Mon" />
    P.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bowman</span>,&#32;
    <meta itemprop="givenName" content="Samuel R." />
    S.</span>
  &#32;
    (<span itemprop="datePublished">2021</span>).
  &#32;<span itemprop="name">BBQ: A hand-built bias benchmark for question answering</span>.<i>
    <span itemprop="about">arXiv preprint arXiv:2110.08193</span></i>.&#32;Retrieved from&#32;
  <a href="https://arxiv.org/abs/2110.08193"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://arxiv.org/abs/2110.08193</a></span>




</span></span>)</span> can be work intensive to get up and running. There are reports of the setup of BBQ requiring a software engineer a full work week 
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#ganguli2023challengesevaluatingai"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Deep"><span itemprop="familyName">Ganguli</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Nicholas"><span itemprop="familyName">Schiefer</span></span>
                et al.,&#32;<span itemprop="datePublished">2023</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/WebPage"
      data-type="webpage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ganguli</span>,&#32;
    <meta itemprop="givenName" content="Deep" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Schiefer</span>,&#32;
    <meta itemprop="givenName" content="Nicholas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Favaro</span>,&#32;
    <meta itemprop="givenName" content="Marina" />
    M.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Clark</span>,&#32;
    <meta itemprop="givenName" content="Jack" />
    J.</span>(<span itemprop="datePublished">2023,&#32;10</span>).&#32;Retrieved from&#32;
  <a href="https://www.anthropic.com/index/evaluating-ai-systems"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://www.anthropic.com/index/evaluating-ai-systems</a></span>




</span></span>)</span>. This prevents more resource-strapped users from running these benchmarks in the first place. Similarly, many benchmarks require a large amount of compute for model inference to use them. If you didn&rsquo;t trust the reported results and wanted to test your OpenAI API on the standard <em>HELM</em> benchmark 
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#liang2022holisticevaluationlanguage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Percy"><span itemprop="familyName">Liang</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Rishi"><span itemprop="familyName">Bommasani</span></span>
                et al.,&#32;<span itemprop="datePublished">2022</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Liang</span>,&#32;
    <meta itemprop="givenName" content="Percy" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bommasani</span>,&#32;
    <meta itemprop="givenName" content="Rishi" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Lee</span>,&#32;
    <meta itemprop="givenName" content="Tony" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Tsipras</span>,&#32;
    <meta itemprop="givenName" content="Dimitris" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Soylu</span>,&#32;
    <meta itemprop="givenName" content="Dilara" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yasunaga</span>,&#32;
    <meta itemprop="givenName" content="Michihiro" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
    <meta itemprop="givenName" content="Yian" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Narayanan</span>,&#32;
    <meta itemprop="givenName" content="Deepak" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wu</span>,&#32;
    <meta itemprop="givenName" content="Yuhuai" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kumar</span>,&#32;
    <meta itemprop="givenName" content="Ananya" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Newman</span>,&#32;
    <meta itemprop="givenName" content="Benjamin" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yuan</span>,&#32;
    <meta itemprop="givenName" content="Binhang" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yan</span>,&#32;
    <meta itemprop="givenName" content="Bobby" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
    <meta itemprop="givenName" content="Ce" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cosgrove</span>,&#32;
    <meta itemprop="givenName" content="Christian" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Manning</span>,&#32;
    <meta itemprop="givenName" content="Christopher D." />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ré</span>,&#32;
    <meta itemprop="givenName" content="Christopher" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Acosta-Navas</span>,&#32;
    <meta itemprop="givenName" content="Diana" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hudson</span>,&#32;
    <meta itemprop="givenName" content="Drew A." />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zelikman</span>,&#32;
    <meta itemprop="givenName" content="Eric" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Durmus</span>,&#32;
    <meta itemprop="givenName" content="Esin" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ladhak</span>,&#32;
    <meta itemprop="givenName" content="Faisal" />
    F.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Rong</span>,&#32;
    <meta itemprop="givenName" content="Frieda" />
    F.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ren</span>,&#32;
    <meta itemprop="givenName" content="Hongyu" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yao</span>,&#32;
    <meta itemprop="givenName" content="Huaxiu" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
    <meta itemprop="givenName" content="Jue" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Santhanam</span>,&#32;
    <meta itemprop="givenName" content="Keshav" />
    K.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Orr</span>,&#32;
    <meta itemprop="givenName" content="Laurel" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zheng</span>,&#32;
    <meta itemprop="givenName" content="Lucia" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yuksekgonul</span>,&#32;
    <meta itemprop="givenName" content="Mert" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Suzgun</span>,&#32;
    <meta itemprop="givenName" content="Mirac" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kim</span>,&#32;
    <meta itemprop="givenName" content="Nathan" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Guha</span>,&#32;
    <meta itemprop="givenName" content="Neel" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chatterji</span>,&#32;
    <meta itemprop="givenName" content="Niladri" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Khattab</span>,&#32;
    <meta itemprop="givenName" content="Omar" />
    O.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Henderson</span>,&#32;
    <meta itemprop="givenName" content="Peter" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Huang</span>,&#32;
    <meta itemprop="givenName" content="Qian" />
    Q.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chi</span>,&#32;
    <meta itemprop="givenName" content="Ryan" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xie</span>,&#32;
    <meta itemprop="givenName" content="Sang Michael" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Santurkar</span>,&#32;
    <meta itemprop="givenName" content="Shibani" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ganguli</span>,&#32;
    <meta itemprop="givenName" content="Surya" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hashimoto</span>,&#32;
    <meta itemprop="givenName" content="Tatsunori" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Icard</span>,&#32;
    <meta itemprop="givenName" content="Thomas" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
    <meta itemprop="givenName" content="Tianyi" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chaudhary</span>,&#32;
    <meta itemprop="givenName" content="Vishrav" />
    V.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
    <meta itemprop="givenName" content="William" />
    W.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
    <meta itemprop="givenName" content="Xuechen" />
    X.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mai</span>,&#32;
    <meta itemprop="givenName" content="Yifan" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
    <meta itemprop="givenName" content="Yuhui" />
    Y.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Koreeda</span>,&#32;
    <meta itemprop="givenName" content="Yuta" />
    Y.</span>
  &#32;
    (<span itemprop="datePublished">2022</span>).
  &#32;<span itemprop="name">Holistic Evaluation of Language Models</span>.
  <a href="https://doi.org/10.48550/arXiv.2211.09110"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2211.09110</a></span>




</span></span>)</span>, you may have to spend a lot of money (<a href="https://crfm.stanford.edu/helm/lite/">HELM lite</a> is an interesting alternative).</p>



<figure style="text-align: center;"><img
         src="img/06_expensive_v1.png" width="90%"style="display:inline"
    /><figcaption>
        <p align="center">illustration of how some benchmarks are expensive to run.</p>
    </figcaption>
</figure>
<hr>
<h2 id="3-problems-with-benchmark-ecosystem">3. Problems with benchmark ecosystem<a hidden class="anchor" aria-hidden="true" href="#3-problems-with-benchmark-ecosystem">#</a></h2>
<h3 id="problem-5-missing-benchmarks">Problem 5: <em>missing</em> benchmarks<a hidden class="anchor" aria-hidden="true" href="#problem-5-missing-benchmarks">#</a></h3>
<p><em>For many model qualities, benchmarks could be created but haven&rsquo;t been created yet.</em> There is an infinite number of possible model qualities to evaluate. For any given benchmark, it is easy to construct an adjacent model quality that is not yet measured by a benchmark. For example, whilst HumanEval aims to evaluate <em>&ldquo;ability to write Python code&rdquo;</em>, we may instead care about <em>&ldquo;ability to write Python code using framework X&rdquo;</em>. As benchmarks are typically handcrafted, most LM qualities do not have a corresponding existing benchmark – a benchmark for those qualities is <em>missing</em>. For multi-use systems like LLMs, developers are frequently exploring new use-cases with no corresponding benchmarks. The figure below illustrates how most model qualities do not have a corresponding benchmark.</p>



<figure style="text-align: center;"><img
         src="img/01_missing_v2.png" width="90%"style="display:inline"
    /><figcaption>
        <p align="center">Illustration of how for many model qualities the corresponding benchmarks are missing.</p>
    </figcaption>
</figure>
<h3 id="problem-6-unfeasible-benchmarks">Problem 6: <em>unfeasible</em> benchmarks<a hidden class="anchor" aria-hidden="true" href="#problem-6-unfeasible-benchmarks">#</a></h3>
<p><em>Some model qualities are too difficult to measure with current methods.</em> Whilst HumanEval evaluates writing short Python functions, the job of a software engineer involves many other tasks, including <em>creating pull-requests</em>, <em>co-ordinating with a team</em>, or <em>coming up with a good system architecture</em>. Due to the complexity of testing some of these tasks, no benchmark exists that captures all model qualities required for an LM to take over an software engineering role. The field of <em>scalable oversight</em> 
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#amodei2016concreteproblemsai"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Dario"><span itemprop="familyName">Amodei</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Chris"><span itemprop="familyName">Olah</span></span>
                et al.,&#32;<span itemprop="datePublished">2016</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Amodei</span>,&#32;
    <meta itemprop="givenName" content="Dario" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Olah</span>,&#32;
    <meta itemprop="givenName" content="Chris" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Steinhardt</span>,&#32;
    <meta itemprop="givenName" content="Jacob" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Christiano</span>,&#32;
    <meta itemprop="givenName" content="Paul" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Schulman</span>,&#32;
    <meta itemprop="givenName" content="John" />
    J.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mané</span>,&#32;
    <meta itemprop="givenName" content="Dan" />
    D.</span>
  &#32;
    (<span itemprop="datePublished">2016</span>).
  &#32;<span itemprop="name">Concrete Problems in AI Safety</span>.
  <a href="https://doi.org/10.48550/arXiv.1606.06565"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.1606.06565</a></span>




</span></span>)</span> aims to tackle the problem of evaluating increasingly complex model qualities, especially when models exhibit qualities that surpass humans capabilities.</p>



<figure style="text-align: center;"><img
         src="img/02_unfeasible_v1.png" width="90%"style="display:inline"
    /><figcaption>
        <p align="center">Illustration of how many model qualities are too complex to evaluate with current benchmark methods.</p>
    </figcaption>
</figure>
<h3 id="problem-7-aimless-benchmarks">Problem 7: <em>aimless</em> benchmarks<a hidden class="anchor" aria-hidden="true" href="#problem-7-aimless-benchmarks">#</a></h3>
<p><em>Some users do not know what qualities they want their LM to have</em>. Unlike previous ML systems, LMs have become truly <em>multi-use</em>. The same model may be used as writing assistant, translator, therapist, or code autocomplete engine. Due to the open-ended nature of model uses, model developers struggle to explicitly state what <em>model qualities</em> they aim for. Qualities tend to be very ambiguous, such as <em>harmless</em> and <em>helpful</em>. Without clarity around desirable model qualities, it is impossible to identify or create a suitable benchmark. Model developers rely heavily on <em>red teaming</em> as opposed to automatic benchmarks – as this more closely aligns with model use. I believe the reason why traditional benchmarks are not held in high regard by model developer, is primarily because they require the developers to decide what&rsquo;s important – what qualities to test. This issue was well summarized in <a href="https://open.spotify.com/episode/5CV9ongsBGbjtF5SgfgDNF?si=cDhbu8KET5OIB7XdsfXI6w&amp;t=1138&amp;context=spotify%3Ashow%3A6onNcSqsP6hEEqmZ6TU2g8">this podcast</a> 
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#bashir20232023ainathan"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Daniel"><span itemprop="familyName">Bashir</span></span>,&#32;<span itemprop="datePublished">2023</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/CreativeWork"
      data-type="song"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bashir</span>,&#32;
    <meta itemprop="givenName" content="Daniel" />
    D.</span>
  &#32;
    (<span itemprop="datePublished">2023</span>).
  &#32;<span itemprop="name">
    <i>2023 in AI, with Nathan Benaich</i></span>.
  &#32;Retrieved from&#32;
  <a href="https://open.spotify.com/episode/5CV9ongsBGbjtF5SgfgDNF?si=cDhbu8KET5OIB7XdsfXI6w&amp;t=1138&amp;context=spotify%3Ashow%3A6onNcSqsP6hEEqmZ6TU2g8"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://open.spotify.com/episode/5CV9ongsBGbjtF5SgfgDNF?si=cDhbu8KET5OIB7XdsfXI6w&amp;t=1138&amp;context=spotify%3Ashow%3A6onNcSqsP6hEEqmZ6TU2g8</a></span>

</span></span>)</span>. Red teaming allows model developers to implicitly let the red team members decide what model qualities to test, instead of the developers having to state them explicitly.</p>



<figure style="text-align: center;"><img
         src="img/04_aimless_v1.png" width="90%"style="display:inline"
    /><figcaption>
        <p align="center">Illustration of how some benchmark users aimlessly (almost randomly) select benchmarks and model qualities to evaluate. This issue often arises due to a lack of clarity around what qualities matter for their LM application.  </p>
    </figcaption>
</figure>
<hr>
<h2 id="4-conclusion">4. Conclusion<a hidden class="anchor" aria-hidden="true" href="#4-conclusion">#</a></h2>
<p>Whilst my list of problems above certainly is incomplete, I hope it helped provide some clarity around what some of the most important issues of benchmarks are. I believe that it is vital to be aware of these problems whenever <em>using</em> – and especially when <em>developing</em> – benchmarks. Each problem also presents an interesting research direction for future work.</p>



<figure style="text-align: center;"><img
         src="img/08_overview_v0.png" width="90%"style="display:inline"
    /><figcaption>
        <p align="center">Overview of all problems discussed.</p>
    </figcaption>
</figure>
<p>Want to read further? I can recommend Anthropic&rsquo;s blog post on the topic 
  <span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group"><a href="#ganguli2023challengesevaluatingai"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Deep"><span itemprop="familyName">Ganguli</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Nicholas"><span itemprop="familyName">Schiefer</span></span>
                et al.,&#32;<span itemprop="datePublished">2023</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/WebPage"
      data-type="webpage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ganguli</span>,&#32;
    <meta itemprop="givenName" content="Deep" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Schiefer</span>,&#32;
    <meta itemprop="givenName" content="Nicholas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Favaro</span>,&#32;
    <meta itemprop="givenName" content="Marina" />
    M.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Clark</span>,&#32;
    <meta itemprop="givenName" content="Jack" />
    J.</span>(<span itemprop="datePublished">2023,&#32;10</span>).&#32;Retrieved from&#32;
  <a href="https://www.anthropic.com/index/evaluating-ai-systems"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://www.anthropic.com/index/evaluating-ai-systems</a></span>




</span></span>)</span>. If you&rsquo;re looking for a more comprehensive overview over all benchmarks, consider looking at a survey over language model evaluation (e.g. 
  <span class="hugo-cite-intext"
        itemprop="citation"><span class="hugo-cite-group"><a href="#chang2023surveyevaluationlargeb"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Yupeng"><span itemprop="familyName">Chang</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Xu"><span itemprop="familyName">Wang</span></span>
                et al.&#32;(<span itemprop="datePublished">2023</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chang</span>,&#32;
    <meta itemprop="givenName" content="Yupeng" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
    <meta itemprop="givenName" content="Xu" />
    X.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
    <meta itemprop="givenName" content="Jindong" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wu</span>,&#32;
    <meta itemprop="givenName" content="Yuan" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yang</span>,&#32;
    <meta itemprop="givenName" content="Linyi" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhu</span>,&#32;
    <meta itemprop="givenName" content="Kaijie" />
    K.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chen</span>,&#32;
    <meta itemprop="givenName" content="Hao" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yi</span>,&#32;
    <meta itemprop="givenName" content="Xiaoyuan" />
    X.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
    <meta itemprop="givenName" content="Cunxiang" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
    <meta itemprop="givenName" content="Yidong" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ye</span>,&#32;
    <meta itemprop="givenName" content="Wei" />
    W.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
    <meta itemprop="givenName" content="Yue" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chang</span>,&#32;
    <meta itemprop="givenName" content="Yi" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yu</span>,&#32;
    <meta itemprop="givenName" content="Philip S." />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yang</span>,&#32;
    <meta itemprop="givenName" content="Qiang" />
    Q.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xie</span>,&#32;
    <meta itemprop="givenName" content="Xing" />
    X.</span>
  &#32;
    (<span itemprop="datePublished">2023</span>).
  &#32;<span itemprop="name">A Survey on Evaluation of Large Language Models</span>.
  <a href="https://doi.org/10.48550/arXiv.2307.03109"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2307.03109</a></span>




</span></span>;&#32;<span class="hugo-cite-group"><a href="#guo2023evaluatinglargelanguage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Zishan"><span itemprop="familyName">Guo</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Renren"><span itemprop="familyName">Jin</span></span>
                et al.&#32;(<span itemprop="datePublished">2023</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Guo</span>,&#32;
    <meta itemprop="givenName" content="Zishan" />
    Z.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Jin</span>,&#32;
    <meta itemprop="givenName" content="Renren" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Liu</span>,&#32;
    <meta itemprop="givenName" content="Chuang" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Huang</span>,&#32;
    <meta itemprop="givenName" content="Yufei" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Shi</span>,&#32;
    <meta itemprop="givenName" content="Dan" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Supryadi</span></span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yu</span>,&#32;
    <meta itemprop="givenName" content="Linhao" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Liu</span>,&#32;
    <meta itemprop="givenName" content="Yan" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
    <meta itemprop="givenName" content="Jiaxuan" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xiong</span>,&#32;
    <meta itemprop="givenName" content="Bojian" />
    B.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xiong</span>,&#32;
    <meta itemprop="givenName" content="Deyi" />
    D.</span>
  &#32;
    (<span itemprop="datePublished">2023</span>).
  &#32;<span itemprop="name">Evaluating Large Language Models: A Comprehensive Survey</span>.
  <a href="https://doi.org/10.48550/arXiv.2310.19736"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2310.19736</a></span>




</span></span>)</span>.)</p>
<h2 id="acknowledgements">Acknowledgements<a hidden class="anchor" aria-hidden="true" href="#acknowledgements">#</a></h2>
<p>Big thanks to the many people I have been extensively discussing benchmarks with, especially Samuel Albanie, Rob Mullins and Benjamin Minixhofer. I would also like to highlight the useful discussions I had at the <a href="https://sites.google.com/view/fm-eval-workshop/home">Evaluating Foundation/Frontier Models Workshop</a> organised by Emanuele La Malfa et al. – this helped clarify my thoughts.</p>
<h2 id="versions">Versions<a hidden class="anchor" aria-hidden="true" href="#versions">#</a></h2>
<ul>
<li><strong>v1</strong> (2024-01-15): initial public draft release</li>
</ul>
<hr>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>

  

  










<section class="hugo-cite-bibliography">
  <ol>
    

      <div id="amodei2016concreteproblemsai">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Amodei</span>,&#32;
    <meta itemprop="givenName" content="Dario" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Olah</span>,&#32;
    <meta itemprop="givenName" content="Chris" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Steinhardt</span>,&#32;
    <meta itemprop="givenName" content="Jacob" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Christiano</span>,&#32;
    <meta itemprop="givenName" content="Paul" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Schulman</span>,&#32;
    <meta itemprop="givenName" content="John" />
    J.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mané</span>,&#32;
    <meta itemprop="givenName" content="Dan" />
    D.</span>
  &#32;
    (<span itemprop="datePublished">2016</span>).
  &#32;<span itemprop="name">Concrete Problems in AI Safety</span>.
  <a href="https://doi.org/10.48550/arXiv.1606.06565"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.1606.06565</a></span>




</li>

      </div>

      <div id="bashir20232023ainathan">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/CreativeWork"
      data-type="song"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bashir</span>,&#32;
    <meta itemprop="givenName" content="Daniel" />
    D.</span>
  &#32;
    (<span itemprop="datePublished">2023</span>).
  &#32;<span itemprop="name">
    <i>2023 in AI, with Nathan Benaich</i></span>.
  &#32;Retrieved from&#32;
  <a href="https://open.spotify.com/episode/5CV9ongsBGbjtF5SgfgDNF?si=cDhbu8KET5OIB7XdsfXI6w&amp;t=1138&amp;context=spotify%3Ashow%3A6onNcSqsP6hEEqmZ6TU2g8"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://open.spotify.com/episode/5CV9ongsBGbjtF5SgfgDNF?si=cDhbu8KET5OIB7XdsfXI6w&amp;t=1138&amp;context=spotify%3Ashow%3A6onNcSqsP6hEEqmZ6TU2g8</a></span>

</li>

      </div>

      <div id="bojar2015findings2015workshop">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bojar</span>,&#32;
    <meta itemprop="givenName" content="Ondrej" />
    O.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chatterjee</span>,&#32;
    <meta itemprop="givenName" content="Rajen" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Federmann</span>,&#32;
    <meta itemprop="givenName" content="Christian" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Haddow</span>,&#32;
    <meta itemprop="givenName" content="Barry" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Huck</span>,&#32;
    <meta itemprop="givenName" content="Matthias" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hokamp</span>,&#32;
    <meta itemprop="givenName" content="Chris" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Koehn</span>,&#32;
    <meta itemprop="givenName" content="Philipp" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Logacheva</span>,&#32;
    <meta itemprop="givenName" content="Varvara" />
    V.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Monz</span>,&#32;
    <meta itemprop="givenName" content="Christof" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Negri</span>,&#32;
    <meta itemprop="givenName" content="Matteo" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Post</span>,&#32;
    <meta itemprop="givenName" content="Matt" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Scarton</span>,&#32;
    <meta itemprop="givenName" content="Carolina" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Specia</span>,&#32;
    <meta itemprop="givenName" content="Lucia" />
    L.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Turchi</span>,&#32;
    <meta itemprop="givenName" content="Marco" />
    M.</span>
  &#32;
    (<span itemprop="datePublished">2015</span>).
  &#32;<span itemprop="name">Findings of the 2015 Workshop on Statistical Machine Translation</span>.<i>
    <span itemprop="about">Proceedings of the Tenth Workshop on Statistical Machine Translation</span></i>.&#32;<span itemprop="pagination">1–46</span>.&#32;Retrieved from&#32;
  <a href="https://www.research.ed.ac.uk/en/publications/findings-of-the-2015-workshop-on-statistical-machine-translation"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://www.research.ed.ac.uk/en/publications/findings-of-the-2015-workshop-on-statistical-machine-translation</a></span>




</li>

      </div>

      <div id="chang2023surveyevaluationlargeb">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chang</span>,&#32;
    <meta itemprop="givenName" content="Yupeng" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
    <meta itemprop="givenName" content="Xu" />
    X.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
    <meta itemprop="givenName" content="Jindong" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wu</span>,&#32;
    <meta itemprop="givenName" content="Yuan" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yang</span>,&#32;
    <meta itemprop="givenName" content="Linyi" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhu</span>,&#32;
    <meta itemprop="givenName" content="Kaijie" />
    K.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chen</span>,&#32;
    <meta itemprop="givenName" content="Hao" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yi</span>,&#32;
    <meta itemprop="givenName" content="Xiaoyuan" />
    X.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
    <meta itemprop="givenName" content="Cunxiang" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
    <meta itemprop="givenName" content="Yidong" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ye</span>,&#32;
    <meta itemprop="givenName" content="Wei" />
    W.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
    <meta itemprop="givenName" content="Yue" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chang</span>,&#32;
    <meta itemprop="givenName" content="Yi" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yu</span>,&#32;
    <meta itemprop="givenName" content="Philip S." />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yang</span>,&#32;
    <meta itemprop="givenName" content="Qiang" />
    Q.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xie</span>,&#32;
    <meta itemprop="givenName" content="Xing" />
    X.</span>
  &#32;
    (<span itemprop="datePublished">2023</span>).
  &#32;<span itemprop="name">A Survey on Evaluation of Large Language Models</span>.
  <a href="https://doi.org/10.48550/arXiv.2307.03109"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2307.03109</a></span>




</li>

      </div>

      <div id="chen2021evaluatinglargelanguage">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chen</span>,&#32;
    <meta itemprop="givenName" content="Mark" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Tworek</span>,&#32;
    <meta itemprop="givenName" content="Jerry" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Jun</span>,&#32;
    <meta itemprop="givenName" content="Heewoo" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yuan</span>,&#32;
    <meta itemprop="givenName" content="Qiming" />
    Q.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Pinto</span>,&#32;
    <meta itemprop="givenName" content="Henrique Ponde de Oliveira" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kaplan</span>,&#32;
    <meta itemprop="givenName" content="Jared" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Edwards</span>,&#32;
    <meta itemprop="givenName" content="Harri" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Burda</span>,&#32;
    <meta itemprop="givenName" content="Yuri" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Joseph</span>,&#32;
    <meta itemprop="givenName" content="Nicholas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Brockman</span>,&#32;
    <meta itemprop="givenName" content="Greg" />
    G.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ray</span>,&#32;
    <meta itemprop="givenName" content="Alex" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Puri</span>,&#32;
    <meta itemprop="givenName" content="Raul" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Krueger</span>,&#32;
    <meta itemprop="givenName" content="Gretchen" />
    G.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Petrov</span>,&#32;
    <meta itemprop="givenName" content="Michael" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Khlaaf</span>,&#32;
    <meta itemprop="givenName" content="Heidy" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sastry</span>,&#32;
    <meta itemprop="givenName" content="Girish" />
    G.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mishkin</span>,&#32;
    <meta itemprop="givenName" content="Pamela" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chan</span>,&#32;
    <meta itemprop="givenName" content="Brooke" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gray</span>,&#32;
    <meta itemprop="givenName" content="Scott" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ryder</span>,&#32;
    <meta itemprop="givenName" content="Nick" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Pavlov</span>,&#32;
    <meta itemprop="givenName" content="Mikhail" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Power</span>,&#32;
    <meta itemprop="givenName" content="Alethea" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kaiser</span>,&#32;
    <meta itemprop="givenName" content="Lukasz" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bavarian</span>,&#32;
    <meta itemprop="givenName" content="Mohammad" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Winter</span>,&#32;
    <meta itemprop="givenName" content="Clemens" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Tillet</span>,&#32;
    <meta itemprop="givenName" content="Philippe" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Such</span>,&#32;
    <meta itemprop="givenName" content="Felipe Petroski" />
    F.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cummings</span>,&#32;
    <meta itemprop="givenName" content="Dave" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Plappert</span>,&#32;
    <meta itemprop="givenName" content="Matthias" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chantzis</span>,&#32;
    <meta itemprop="givenName" content="Fotios" />
    F.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Barnes</span>,&#32;
    <meta itemprop="givenName" content="Elizabeth" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Herbert-Voss</span>,&#32;
    <meta itemprop="givenName" content="Ariel" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Guss</span>,&#32;
    <meta itemprop="givenName" content="William Hebgen" />
    W.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Nichol</span>,&#32;
    <meta itemprop="givenName" content="Alex" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Paino</span>,&#32;
    <meta itemprop="givenName" content="Alex" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Tezak</span>,&#32;
    <meta itemprop="givenName" content="Nikolas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Tang</span>,&#32;
    <meta itemprop="givenName" content="Jie" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Babuschkin</span>,&#32;
    <meta itemprop="givenName" content="Igor" />
    I.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Balaji</span>,&#32;
    <meta itemprop="givenName" content="Suchir" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Jain</span>,&#32;
    <meta itemprop="givenName" content="Shantanu" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Saunders</span>,&#32;
    <meta itemprop="givenName" content="William" />
    W.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hesse</span>,&#32;
    <meta itemprop="givenName" content="Christopher" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Carr</span>,&#32;
    <meta itemprop="givenName" content="Andrew N." />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Leike</span>,&#32;
    <meta itemprop="givenName" content="Jan" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Achiam</span>,&#32;
    <meta itemprop="givenName" content="Josh" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Misra</span>,&#32;
    <meta itemprop="givenName" content="Vedant" />
    V.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Morikawa</span>,&#32;
    <meta itemprop="givenName" content="Evan" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Radford</span>,&#32;
    <meta itemprop="givenName" content="Alec" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Knight</span>,&#32;
    <meta itemprop="givenName" content="Matthew" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Brundage</span>,&#32;
    <meta itemprop="givenName" content="Miles" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Murati</span>,&#32;
    <meta itemprop="givenName" content="Mira" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mayer</span>,&#32;
    <meta itemprop="givenName" content="Katie" />
    K.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Welinder</span>,&#32;
    <meta itemprop="givenName" content="Peter" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">McGrew</span>,&#32;
    <meta itemprop="givenName" content="Bob" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Amodei</span>,&#32;
    <meta itemprop="givenName" content="Dario" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">McCandlish</span>,&#32;
    <meta itemprop="givenName" content="Sam" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sutskever</span>,&#32;
    <meta itemprop="givenName" content="Ilya" />
    I.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zaremba</span>,&#32;
    <meta itemprop="givenName" content="Wojciech" />
    W.</span>
  &#32;
    (<span itemprop="datePublished">2021</span>).
  &#32;<span itemprop="name">Evaluating Large Language Models Trained on Code</span>.
  <a href="https://doi.org/10.48550/arXiv.2107.03374"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2107.03374</a></span>




</li>

      </div>

      <div id="ganguli2022redteaminglanguage">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ganguli</span>,&#32;
    <meta itemprop="givenName" content="Deep" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Lovitt</span>,&#32;
    <meta itemprop="givenName" content="Liane" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kernion</span>,&#32;
    <meta itemprop="givenName" content="Jackson" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Askell</span>,&#32;
    <meta itemprop="givenName" content="Amanda" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bai</span>,&#32;
    <meta itemprop="givenName" content="Yuntao" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kadavath</span>,&#32;
    <meta itemprop="givenName" content="Saurav" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mann</span>,&#32;
    <meta itemprop="givenName" content="Ben" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Perez</span>,&#32;
    <meta itemprop="givenName" content="Ethan" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Schiefer</span>,&#32;
    <meta itemprop="givenName" content="Nicholas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ndousse</span>,&#32;
    <meta itemprop="givenName" content="Kamal" />
    K.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Jones</span>,&#32;
    <meta itemprop="givenName" content="Andy" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bowman</span>,&#32;
    <meta itemprop="givenName" content="Sam" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chen</span>,&#32;
    <meta itemprop="givenName" content="Anna" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Conerly</span>,&#32;
    <meta itemprop="givenName" content="Tom" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">DasSarma</span>,&#32;
    <meta itemprop="givenName" content="Nova" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Drain</span>,&#32;
    <meta itemprop="givenName" content="Dawn" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Elhage</span>,&#32;
    <meta itemprop="givenName" content="Nelson" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">El-Showk</span>,&#32;
    <meta itemprop="givenName" content="Sheer" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Fort</span>,&#32;
    <meta itemprop="givenName" content="Stanislav" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hatfield-Dodds</span>,&#32;
    <meta itemprop="givenName" content="Zac" />
    Z.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Henighan</span>,&#32;
    <meta itemprop="givenName" content="Tom" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hernandez</span>,&#32;
    <meta itemprop="givenName" content="Danny" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hume</span>,&#32;
    <meta itemprop="givenName" content="Tristan" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Jacobson</span>,&#32;
    <meta itemprop="givenName" content="Josh" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Johnston</span>,&#32;
    <meta itemprop="givenName" content="Scott" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kravec</span>,&#32;
    <meta itemprop="givenName" content="Shauna" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Olsson</span>,&#32;
    <meta itemprop="givenName" content="Catherine" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ringer</span>,&#32;
    <meta itemprop="givenName" content="Sam" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Tran-Johnson</span>,&#32;
    <meta itemprop="givenName" content="Eli" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Amodei</span>,&#32;
    <meta itemprop="givenName" content="Dario" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Brown</span>,&#32;
    <meta itemprop="givenName" content="Tom" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Joseph</span>,&#32;
    <meta itemprop="givenName" content="Nicholas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">McCandlish</span>,&#32;
    <meta itemprop="givenName" content="Sam" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Olah</span>,&#32;
    <meta itemprop="givenName" content="Chris" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kaplan</span>,&#32;
    <meta itemprop="givenName" content="Jared" />
    J.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Clark</span>,&#32;
    <meta itemprop="givenName" content="Jack" />
    J.</span>
  &#32;
    (<span itemprop="datePublished">2022</span>).
  &#32;<span itemprop="name">Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned</span>.
  <a href="https://doi.org/10.48550/arXiv.2209.07858"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2209.07858</a></span>




</li>

      </div>

      <div id="ganguli2023challengesevaluatingai">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/WebPage"
      data-type="webpage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ganguli</span>,&#32;
    <meta itemprop="givenName" content="Deep" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Schiefer</span>,&#32;
    <meta itemprop="givenName" content="Nicholas" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Favaro</span>,&#32;
    <meta itemprop="givenName" content="Marina" />
    M.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Clark</span>,&#32;
    <meta itemprop="givenName" content="Jack" />
    J.</span>(<span itemprop="datePublished">2023,&#32;10</span>).&#32;Retrieved from&#32;
  <a href="https://www.anthropic.com/index/evaluating-ai-systems"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://www.anthropic.com/index/evaluating-ai-systems</a></span>




</li>

      </div>

      <div id="guo2023evaluatinglargelanguage">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Guo</span>,&#32;
    <meta itemprop="givenName" content="Zishan" />
    Z.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Jin</span>,&#32;
    <meta itemprop="givenName" content="Renren" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Liu</span>,&#32;
    <meta itemprop="givenName" content="Chuang" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Huang</span>,&#32;
    <meta itemprop="givenName" content="Yufei" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Shi</span>,&#32;
    <meta itemprop="givenName" content="Dan" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Supryadi</span></span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yu</span>,&#32;
    <meta itemprop="givenName" content="Linhao" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Liu</span>,&#32;
    <meta itemprop="givenName" content="Yan" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
    <meta itemprop="givenName" content="Jiaxuan" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xiong</span>,&#32;
    <meta itemprop="givenName" content="Bojian" />
    B.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xiong</span>,&#32;
    <meta itemprop="givenName" content="Deyi" />
    D.</span>
  &#32;
    (<span itemprop="datePublished">2023</span>).
  &#32;<span itemprop="name">Evaluating Large Language Models: A Comprehensive Survey</span>.
  <a href="https://doi.org/10.48550/arXiv.2310.19736"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2310.19736</a></span>




</li>

      </div>

      <div id="lamalfa2023arrtlanguagemodelsasaserviceoverview">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">La Malfa</span>,&#32;
    <meta itemprop="givenName" content="Emanuele" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Petrov</span>,&#32;
    <meta itemprop="givenName" content="Aleksandar" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Frieder</span>,&#32;
    <meta itemprop="givenName" content="Simon" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Weinhuber</span>,&#32;
    <meta itemprop="givenName" content="Christoph" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Burnell</span>,&#32;
    <meta itemprop="givenName" content="Ryan" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cohn</span>,&#32;
    <meta itemprop="givenName" content="Anthony G." />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Shadbolt</span>,&#32;
    <meta itemprop="givenName" content="Nigel" />
    N.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wooldridge</span>,&#32;
    <meta itemprop="givenName" content="Michael" />
    M.</span>
  &#32;
    (<span itemprop="datePublished">2023</span>).
  &#32;<span itemprop="name">The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges</span>.
  <a href="https://doi.org/10.48550/arXiv.2309.16573"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2309.16573</a></span>




</li>

      </div>

      <div id="liang2022holisticevaluationlanguage">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Liang</span>,&#32;
    <meta itemprop="givenName" content="Percy" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bommasani</span>,&#32;
    <meta itemprop="givenName" content="Rishi" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Lee</span>,&#32;
    <meta itemprop="givenName" content="Tony" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Tsipras</span>,&#32;
    <meta itemprop="givenName" content="Dimitris" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Soylu</span>,&#32;
    <meta itemprop="givenName" content="Dilara" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yasunaga</span>,&#32;
    <meta itemprop="givenName" content="Michihiro" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
    <meta itemprop="givenName" content="Yian" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Narayanan</span>,&#32;
    <meta itemprop="givenName" content="Deepak" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wu</span>,&#32;
    <meta itemprop="givenName" content="Yuhuai" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kumar</span>,&#32;
    <meta itemprop="givenName" content="Ananya" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Newman</span>,&#32;
    <meta itemprop="givenName" content="Benjamin" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yuan</span>,&#32;
    <meta itemprop="givenName" content="Binhang" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yan</span>,&#32;
    <meta itemprop="givenName" content="Bobby" />
    B.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
    <meta itemprop="givenName" content="Ce" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cosgrove</span>,&#32;
    <meta itemprop="givenName" content="Christian" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Manning</span>,&#32;
    <meta itemprop="givenName" content="Christopher D." />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ré</span>,&#32;
    <meta itemprop="givenName" content="Christopher" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Acosta-Navas</span>,&#32;
    <meta itemprop="givenName" content="Diana" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hudson</span>,&#32;
    <meta itemprop="givenName" content="Drew A." />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zelikman</span>,&#32;
    <meta itemprop="givenName" content="Eric" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Durmus</span>,&#32;
    <meta itemprop="givenName" content="Esin" />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ladhak</span>,&#32;
    <meta itemprop="givenName" content="Faisal" />
    F.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Rong</span>,&#32;
    <meta itemprop="givenName" content="Frieda" />
    F.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ren</span>,&#32;
    <meta itemprop="givenName" content="Hongyu" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yao</span>,&#32;
    <meta itemprop="givenName" content="Huaxiu" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
    <meta itemprop="givenName" content="Jue" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Santhanam</span>,&#32;
    <meta itemprop="givenName" content="Keshav" />
    K.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Orr</span>,&#32;
    <meta itemprop="givenName" content="Laurel" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zheng</span>,&#32;
    <meta itemprop="givenName" content="Lucia" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yuksekgonul</span>,&#32;
    <meta itemprop="givenName" content="Mert" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Suzgun</span>,&#32;
    <meta itemprop="givenName" content="Mirac" />
    M.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kim</span>,&#32;
    <meta itemprop="givenName" content="Nathan" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Guha</span>,&#32;
    <meta itemprop="givenName" content="Neel" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chatterji</span>,&#32;
    <meta itemprop="givenName" content="Niladri" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Khattab</span>,&#32;
    <meta itemprop="givenName" content="Omar" />
    O.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Henderson</span>,&#32;
    <meta itemprop="givenName" content="Peter" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Huang</span>,&#32;
    <meta itemprop="givenName" content="Qian" />
    Q.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chi</span>,&#32;
    <meta itemprop="givenName" content="Ryan" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xie</span>,&#32;
    <meta itemprop="givenName" content="Sang Michael" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Santurkar</span>,&#32;
    <meta itemprop="givenName" content="Shibani" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ganguli</span>,&#32;
    <meta itemprop="givenName" content="Surya" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hashimoto</span>,&#32;
    <meta itemprop="givenName" content="Tatsunori" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Icard</span>,&#32;
    <meta itemprop="givenName" content="Thomas" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
    <meta itemprop="givenName" content="Tianyi" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chaudhary</span>,&#32;
    <meta itemprop="givenName" content="Vishrav" />
    V.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wang</span>,&#32;
    <meta itemprop="givenName" content="William" />
    W.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
    <meta itemprop="givenName" content="Xuechen" />
    X.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mai</span>,&#32;
    <meta itemprop="givenName" content="Yifan" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
    <meta itemprop="givenName" content="Yuhui" />
    Y.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Koreeda</span>,&#32;
    <meta itemprop="givenName" content="Yuta" />
    Y.</span>
  &#32;
    (<span itemprop="datePublished">2022</span>).
  &#32;<span itemprop="name">Holistic Evaluation of Language Models</span>.
  <a href="https://doi.org/10.48550/arXiv.2211.09110"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2211.09110</a></span>




</li>

      </div>

      <div id="microsoft2023planningredteaming">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/WebPage"
      data-type="webpage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Microsoft</span></span>(<span itemprop="datePublished">2023,&#32;11/6</span>).&#32;Retrieved from&#32;
  <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming</a></span>




</li>

      </div>

      <div id="openai2023openairedteaming">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/WebPage"
      data-type="webpage"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">OpenAI</span></span>(<span itemprop="datePublished">2023,&#32;9/19</span>).&#32;Retrieved from&#32;
  <a href="https://openai.com/blog/red-teaming-network"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://openai.com/blog/red-teaming-network</a></span>




</li>

      </div>

      <div id="parrish2021bbqhandbuiltbias">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Parrish</span>,&#32;
    <meta itemprop="givenName" content="Alicia" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chen</span>,&#32;
    <meta itemprop="givenName" content="Angelica" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Nangia</span>,&#32;
    <meta itemprop="givenName" content="Nikita" />
    N.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Padmakumar</span>,&#32;
    <meta itemprop="givenName" content="Vishakh" />
    V.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Phang</span>,&#32;
    <meta itemprop="givenName" content="Jason" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Thompson</span>,&#32;
    <meta itemprop="givenName" content="Jana" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Htut</span>,&#32;
    <meta itemprop="givenName" content="Phu Mon" />
    P.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bowman</span>,&#32;
    <meta itemprop="givenName" content="Samuel R." />
    S.</span>
  &#32;
    (<span itemprop="datePublished">2021</span>).
  &#32;<span itemprop="name">BBQ: A hand-built bias benchmark for question answering</span>.<i>
    <span itemprop="about">arXiv preprint arXiv:2110.08193</span></i>.&#32;Retrieved from&#32;
  <a href="https://arxiv.org/abs/2110.08193"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://arxiv.org/abs/2110.08193</a></span>




</li>

      </div>

      <div id="rudinger2018genderbiascoreference">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Rudinger</span>,&#32;
    <meta itemprop="givenName" content="Rachel" />
    R.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Naradowsky</span>,&#32;
    <meta itemprop="givenName" content="Jason" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Leonard</span>,&#32;
    <meta itemprop="givenName" content="Brian" />
    B.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Van Durme</span>,&#32;
    <meta itemprop="givenName" content="Benjamin" />
    B.</span>
  &#32;
    (<span itemprop="datePublished">2018</span>).
  &#32;<span itemprop="name">Gender Bias in Coreference Resolution</span>.
  <a href="https://doi.org/10.48550/arXiv.1804.09301"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.1804.09301</a></span>




</li>

      </div>

      <div id="yang2023rethinkingbenchmarkcontamination">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Yang</span>,&#32;
    <meta itemprop="givenName" content="Shuo" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chiang</span>,&#32;
    <meta itemprop="givenName" content="Wei-Lin" />
    W.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zheng</span>,&#32;
    <meta itemprop="givenName" content="Lianmin" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gonzalez</span>,&#32;
    <meta itemprop="givenName" content="Joseph E." />
    J.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Stoica</span>,&#32;
    <meta itemprop="givenName" content="Ion" />
    I.</span>
  &#32;
    (<span itemprop="datePublished">2023</span>).
  &#32;<span itemprop="name">Rethinking Benchmark and Contamination for Language Models with Rephrased Samples</span>.
  <a href="https://doi.org/10.48550/arXiv.2311.04850"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2311.04850</a></span>




</li>

      </div>

      <div id="zheng2023judgingllmasajudgemtbench">
        

        <li>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zheng</span>,&#32;
    <meta itemprop="givenName" content="Lianmin" />
    L.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Chiang</span>,&#32;
    <meta itemprop="givenName" content="Wei-Lin" />
    W.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sheng</span>,&#32;
    <meta itemprop="givenName" content="Ying" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhuang</span>,&#32;
    <meta itemprop="givenName" content="Siyuan" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Wu</span>,&#32;
    <meta itemprop="givenName" content="Zhanghao" />
    Z.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhuang</span>,&#32;
    <meta itemprop="givenName" content="Yonghao" />
    Y.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Lin</span>,&#32;
    <meta itemprop="givenName" content="Zi" />
    Z.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
    <meta itemprop="givenName" content="Zhuohan" />
    Z.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
    <meta itemprop="givenName" content="Dacheng" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xing</span>,&#32;
    <meta itemprop="givenName" content="Eric P." />
    E.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Zhang</span>,&#32;
    <meta itemprop="givenName" content="Hao" />
    H.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gonzalez</span>,&#32;
    <meta itemprop="givenName" content="Joseph E." />
    J.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Stoica</span>,&#32;
    <meta itemprop="givenName" content="Ion" />
    I.</span>
  &#32;
    (<span itemprop="datePublished">2023</span>).
  &#32;<span itemprop="name">Judging LLM-as-a-judge with MT-Bench and Chatbot Arena</span>.
  <a href="https://doi.org/10.48550/arXiv.2306.05685"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.48550/arXiv.2306.05685</a></span>




</li>

      </div>
  </ol>
</section>



<hr>
<h2 id="appendix">Appendix<a hidden class="anchor" aria-hidden="true" href="#appendix">#</a></h2>
<h3 id="a-additional-problems">A. Additional problems<a hidden class="anchor" aria-hidden="true" href="#a-additional-problems">#</a></h3>
<p>The previous list is not necessarily exhaustive. Some problems related to benchmarks are not about benchmarks themselves, but the systems we are trying to evaluate. Other problems may be considered sub-problems of one of the mentioned problems.</p>
<ul>
<li>Model stochasticity: difficult to reproduce when using models via LLM-as-a-Service</li>
<li>Brittle benchmarks that can be easily fooled and are not robust to minor prompt pertubations. May be covered by misunderstood and overfitted cases.</li>
<li>(Red teaming lacks standardisation: not about benchmarks, but a common complaint in the community)</li>
</ul>


    




















<h2>Citation</h2>

<p>If you found this post useful for your work, please consider citing it as:

    <blockquote>
<p>Findeis, Arduin. (Jan 2024). The benchmark problems reviving manual evaluation. Retrieved from <a href="https://arduin.io/blog/2024-01-15-benchmark-problems/">https://arduin.io/blog/2024-01-15-benchmark-problems/</a>.</p>
</blockquote>

    or

    <pre tabindex="0"><code> @article{Findeis2023Thebenchmarkproblemsrevivingmanualevaluation,
        title = &#34;The benchmark problems reviving manual evaluation&#34;,
        author = &#34;Findeis, Arduin&#34;,
        journal = &#34;arduin.io&#34;,
        year = &#34;2024&#34;,
        month = &#34;January&#34;,
        url = &#34;https://arduin.io/blog/2024-01-15-benchmark-problems/&#34;
 } 
</code></pre>

</p>
    
  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>
        
        <a href="https://arduin.io/"><img src="/favicon-32x32.png" alt="logo" style="width: 20px; margin: 0 auto;opacity:0.6;"></a>
    </span>
    <span>&copy; 2024 <a href="https://arduin.io/">arduin.io</a></span>
    <span>
        - powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a> with <a href='https://arduin.io/papermod-tweaks/' rel="noopener" target="_blank">tweaks</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script src="/js/table_beautify.js"></script>
</body>

</html>
