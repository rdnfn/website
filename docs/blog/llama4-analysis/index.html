<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What exactly was different about the Chatbot Arena version of Llama 4 Maverick? | arduin.io</title>
<meta name="keywords" content="">
<meta name="description" content="An analysis using the Feedback Forensics app to detect the differences between the Chatbot Arena and publicly released version of Llama 4 Maverick.">
<meta name="author" content="Arduin Findeis">
<link rel="canonical" href="https://arduin.io/blog/llama4-analysis/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.74192b248365a4732a0332cce11ae250dc937c12cac437447139705f932a2159.css" integrity="sha256-dBkrJINlpHMqAzLM4RriUNyTfBLKxDdEcTlwX5MqIVk=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://arduin.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://arduin.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://arduin.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://arduin.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://arduin.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://arduin.io/blog/llama4-analysis/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script data-goatcounter="https://gbqtk.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script><meta property="og:title" content="What exactly was different about the Chatbot Arena version of Llama 4 Maverick?" />
<meta property="og:description" content="An analysis using the Feedback Forensics app to detect the differences between the Chatbot Arena and publicly released version of Llama 4 Maverick." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://arduin.io/blog/llama4-analysis/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2025-04-16T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-04-16T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="What exactly was different about the Chatbot Arena version of Llama 4 Maverick?"/>
<meta name="twitter:description" content="An analysis using the Feedback Forensics app to detect the differences between the Chatbot Arena and publicly released version of Llama 4 Maverick."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blog",
      "item": "https://arduin.io/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What exactly was different about the Chatbot Arena version of Llama 4 Maverick?",
      "item": "https://arduin.io/blog/llama4-analysis/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What exactly was different about the Chatbot Arena version of Llama 4 Maverick?",
  "name": "What exactly was different about the Chatbot Arena version of Llama 4 Maverick?",
  "description": "An analysis using the Feedback Forensics app to detect the differences between the Chatbot Arena and publicly released version of Llama 4 Maverick.",
  "keywords": [
    
  ],
  "articleBody": " üöß This post is currently a public draft with preliminary results. I appreciate any feedback ‚Äì reach out via contact [at] arduin.io! üöß\n1. Introduction The open-weights model Llama 4 Maverick was released on 5 April 2025. Around the same time, a related but non-identical experimental model version was evaluated on Chatbot Arena (Llama-4-Maverick-03-26-Experimental). Some users reported that these two models appear to have notable differences. In this post, I use our Feedback Forensics app to quantitatively dissect how exactly the chat behaviour of the public and the arena models of Llama 4 Maverick differ.\n2. Setup ‚úçÔ∏é Note on naming: for brevity, I will refer to the two versions of Llama 4 Maverick as the public model (used for open-weights release) and arena model (used on Chatbot Arena, full name: Llama-4-Maverick-03-26-Experimental) respectively.\nI don‚Äôt have direct access to the arena model, but the Chatbot Arena team (very helpfully) released a dataset of responses generated by it. With Feedback Forensics we can use this data to directly compare the arena model‚Äôs behaviour to the public model‚Äôs ‚Äî without requiring new responses by the no longer accessible arena model itself (as conventional benchmarks would).\nTo compare the models, I run through the following steps:\nGeneration. Using the public model, I first generate a set of responses to the same prompts for which we have responses from the arena model. The generated public model response are created via Lambda and OpenRouter. Thus, we now have a dataset with three model responses* per prompt: (1) by the arena model, (2) by the public model, and (3) by an opposing (non-Llama-4) model from Chatbot Arena (e.g. GPT-4o).\nBehaviour annotation. Next, for any pair of responses to a prompt, I annotate (with the help of AI annotators) how the observed model behaviours differ across these responses, e.g. which of the responses is more polite. In particular, I test for the 40 different model behaviours included in the ICAI standard set of principles (v3). This ICAI standard set is a collection of instructions (referred to as ‚Äúprinciples‚Äù) for AI annotators to select for model behaviour differences. The collection includes behaviour differences that were previously observed in online discussions, related literature or detected via Inverse Constitutional AI (ICAI). I run principle-following AI annotators on all possible response pairs on a given prompt (e.g. model X‚Äôs vs model Y‚Äôs response) to collect information about how the models differ relative to each other. In the end, we have an extensive collection of annotated response pairs.\nIllustrative example of the resulting dataset: a datapoint in this collection would be a (1) prompt (\"How far away is the moon?\") and (2) a pair of model responses (arena model: \"Great question, very far away: about 384k km\", public model: \"About 384k km\") plus (3) 40 different behaviour annotations labelling which of the responses is more enthusiastic and polite (arena), or concise (public).\nVisualisation. Finally, I locally run the Feedback Forensics app to look at the aggregate annotations results, allowing me to investigate how the behaviour of different models differs across all available prompts and responses. This analysis provides us with a quantitative evaluation of the entire observed model behaviour rather than relying on the small number of individual response pairs I would be able to manually inspect.\n*Some conversations (13.1%) from the Chatbot Arena dataset are multi-turn. For simplicity, I only generated responses for single-turn prompts (86.9% of prompts) with the public model. Thus, for some prompts we only have two responses: by the arena model and the opposing (non-Llama-4) model.\nMetrics Throughout this analysis, we primarily rely on a measure of annotator performance we refer to as annotator strength or simply strength. In our scenario, annotator strength quantifies how much more a particular selected model exhibits a certain behaviour compared to the other models in our dataset (e.g. model X is more polite than the other models). This relative behaviour is measured by testing how consistently a principle-following annotator (e.g. an annotator selecting more polite responses) picks out the selected model‚Äôs responses across the whole set of response pairs from the two (or more) models being compared. If such an annotator always picks the selected model, this indicates that the corresponding behaviour is stronger in the selected model compared to the other models. In other words: strength measures how much selecting for a particular behaviour differentiates the selected model from the others.\nNumerically, strength is defined as the Cohen‚Äôs kappa agreement-level measure of two annotators, a principle-following (e.g. selecting the more polite response) and a model-selecting (e.g. always picking the arena model), weighted by the relevance of the principle-following annotator (e.g. the proportion of comparisons where the principle-following annotator decided that the principle applies).\nIn the context of this analysis, the strength measure can be interpreted as follows:\nStrength¬†metric Interpretation close¬†to¬†+1 The selected model‚Äôs responses exhibit a certain behaviour more than other models.\nExplanation: the behaviour associated with the principle annotator seems to separate model X from other models, as an annotator following exclusively this principle is able to separate the model from the others. close¬†to¬†0 The selected model‚Äôs responses are similar to other models with respect to the annotated behaviour. Explanation: all compared models similarly exhibited a certain behaviour with similar frequency ‚Äî meaning that the principle-following annotator could not distinguish between models. close¬†to¬†-1 The selected model‚Äôs responses exhibit a certain behaviour less than other models. Explanation: other models exhibited the principle‚Äôs behaviour more than the selected model, the principle-following annotator selected those models more than the model of our model-selecting annotator. 3. Results: What‚Äôs different about the arena model? Using this setup, we are now able to get a good overview of how the arena version of Llama 4 Maverick differs from the public version. In this section, I go through the main observations.\nFigure 1: Overview of the Feedback Forensics results. This is a screenshot of the online app.\nFirst and most obvious: Responses are more verbose The arena model‚Äôs responses appear more verbose than the public model‚Äôs. The results in Figure 2 indicate that the arena model generated responses that were ‚Äútoo long‚Äù, ‚Äúmore verbose‚Äù and ‚Äúprovide more detailed explanations‚Äù, relative to the public model‚Äôs responses.\nFigure 2: Results showing that the arena model's responses are more verbose than the public model's. The numbers shown are annotator strength values, see metrics section for a detailed discussion.\n(online version of results)\nThis difference in length is also visible in Feedback Forensics‚Äô Overall Metrics table (Figure 3), which shows more conventional annotation metrics and statistics. When measuring character length, arena model‚Äôs responses are longer relative to the public version for 98.6% of comparisons. The arena model‚Äôs responses are about 6978.4 characters long on average vs 2981.9 for the public model. Note that differences in sampling procedures may play some role in this length difference, but similar differences can also be seen when looking at why the human annotations preferred the arena version in the original dataset (see Figure 7). Whether due to sampling procedures or not, the arena model is rather verbose ‚Äì also relative to other models on Chatbot Arena.\nFigure 3: Screenshot of Feedback Forensics' Overall Metrics table with more conventional metrics showing how much longer the arena model's responses were relative to the public model's. (online version). Second: Tone - friendlier, more enthusiastic, more humour The results show that the tone of the arena model appears to be quite different from the public version: much friendlier, more confident, humorous, emotional, enthusiastic and casual responses. Figure 4 shows the results for a subset of these behaviours - the full set can be found in the online version.\nFigure 4: The arena model appears friendlier, more emotionally expressive, enthusiastic and humorous. Further tone related behaviours were detected but are omitted here to save space, see online version for full results. The numbers shown are annotator strength values, see metrics section for a detailed discussion.\nThird: More formatting and emojis Shown in Figure 5, the results indicate that the arena model more Markdown formatting than the public model: more italics and bold style, more number lists, and more emojis. These observations echo the online observations highlighted by the Chatbot Arena team.\nFigure 5: the arena model uses more formatting than the public model. (online version) Bonus 1: Things that stayed consistent I also find that some behaviour is similar between the models: on the limited set of prompts used in my experiments, the public and arena models are similarly very unlikely to suggest illegal activities, be offensive or use inappropriate language. The shown numbers are strength, but when looking at the relevance of these principles we observe that each is only relevant on 3% or less of the dataset, i.e. neither model frequently produces such content on the tested prompt set.\nFigure 6: The arena and public version are similarly unlikely to produce offensive or inappropriate language, or suggest illegal activities (on the tested prompt set). (online version)\nBonus 2: Human annotators like arena model‚Äôs behaviours Feedback Forensics can not only be used to test model behaviour, but also to see what model behaviour (human) annotators prefer. In this scenario, the strength metric measures how well the principle-following annotators are able to reconstruct the human annotations. In this scenario, a high strength value can be interpreted as meaning that the annotations appear to prefer this model characteristic (or another characteristic that is correlated). For this analysis, I use the original non-tie human annotations from the chatbot Arena dataset (1,774 data points), comparing the arena model to other non-Llama4 models. The results shown below indicate that human annotators indeed like many of these changes in the arena version, potentially explaining the high performance of the arena model.\nFigure 7: Humans annotations indeed appear to select for many of the behaviours stronger in the arena model: friendlier tone, longer, and well-formatted (online version).\n4. Reproducing my analysis There are only three main steps involved in getting to this results locally (after installing ICAI and Feedback Forensics):\nParsing the data into a suitable format: use my parsing notebook here. Running the relevant ICAI experiment with the standard principle annotators, using a config like this one: icai-exp -cd exp/configs/322_arena_llama4_full_pairwise Launching the Feedback Forensics app to inspect the results (this command is also included at the end of the output of the command above): feedback-forensics -d 5. Caveats This post describes preliminary analysis. The usual caveats for AI annotators and potentially inconsistent sampling procedures apply.\nPublic model sampling: I use OpenRouter/Lambda‚Äôs default parameters other than setting temperature to 0. The exact sampling procedure likely differs from the one originally used for the arena model. However, for most relative behavioural observations discussed do not only hold against the public Maverick model ‚Äî but also all the opponent models included in the original dataset. AI annotators: All our annotations are created by LLM-as-a-Judge annotator which can be noisy and have been show to exhibit various biases in different situations. I would expect the exact numbers to vary between annotation runs but the general trends to be the same (e.g. no sign flips in strength for currently high strength results). Nevertheless: it‚Äôs always good advice to also look at (at least a random subset of) the data directly as well! 6. Conclusion Llama 4 Mavericks‚Äôs arena version is notably different from the public open-weight version. It shows many model behaviours that human annotators are known to prefer disproportionately, such as longer responses, more confident responses, and better formatted responses. The strong difference between the two models highlights the importance of having a detailed understanding of preference data beyond a single aggregate number ‚Äì our open-source Feedback Forensics app aims to help with that!\nFurther links If you want to understand your own model and data better, try Feedback Forensics!\nFeedback Forensics: Online version: app.feedbackforensics.com Local version: pip install feedback-forensics GitHub \u0026 docs: github.com/rdnfn/feedback-forensics Resources for reproducing results: Chatbot Arena Llama 4 Maverick data: link to HuggingFace My data parsing notebook: link to file on GitHub My experiment configuration for running ICAI annotators: link file on GitHub ü¶ù\nAcknowledgements I would like to thank Sofia Orellana for detailed feedback on earlier versions of this post. All mistakes remain my own of course.\n",
  "wordCount" : "2042",
  "inLanguage": "en",
  "datePublished": "2025-04-16T00:00:00Z",
  "dateModified": "2025-04-16T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Arduin Findeis"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://arduin.io/blog/llama4-analysis/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "arduin.io",
    "logo": {
      "@type": "ImageObject",
      "url": "https://arduin.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://arduin.io/" accesskey="h" title="arduin.io (Alt + H)">arduin.io</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://arduin.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://arduin.io/research" title="Research">
                    <span>Research</span>
                </a>
            </li>
            <li>
                <a href="https://arduin.io/projects" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://arduin.io/bio" title="Bio">
                    <span>Bio</span>
                </a>
            </li>
            <li>
                <a href="https://arduin.io/blog" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://arduin.io/newsletter" title="Newsletter">
                    <span>Newsletter</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <link rel="stylesheet" type="text/css" href="/hugo-cite.css" />
    
    <h1 class="post-title entry-hint-parent">
      What exactly was different about the Chatbot Arena version of Llama 4 Maverick?
    </h1>
    
    <div class="post-meta"><span title='2025-04-16 00:00:00 +0000 UTC'>April 16, 2025</span>&nbsp;¬∑&nbsp;10 min&nbsp;¬∑&nbsp;Arduin Findeis

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-introduction" aria-label="1. Introduction">1. Introduction</a></li>
                <li>
                    <a href="#2-setup" aria-label="2. Setup">2. Setup</a><ul>
                        
                <li>
                    <a href="#metrics" aria-label="Metrics">Metrics</a></li></ul>
                </li>
                <li>
                    <a href="#3-results-whats-different-about-the-arena-model" aria-label="3. Results: What&rsquo;s different about the arena model?">3. Results: What&rsquo;s different about the arena model?</a><ul>
                        
                <li>
                    <a href="#first-and-most-obvious-responses-are-more-verbose" aria-label="First and most obvious: Responses are more verbose">First and most obvious: Responses are more verbose</a></li>
                <li>
                    <a href="#second-tone---friendlier-more-enthusiastic-more-humour" aria-label="Second: Tone - friendlier, more enthusiastic, more humour">Second: Tone - friendlier, more enthusiastic, more humour</a></li>
                <li>
                    <a href="#third-more-formatting-and-emojis" aria-label="Third: More formatting and emojis">Third: More formatting and emojis</a></li>
                <li>
                    <a href="#bonus-1-things-that-stayed-consistent" aria-label="Bonus 1: Things that stayed consistent">Bonus 1: Things that stayed consistent</a></li>
                <li>
                    <a href="#bonus-2-human-annotators-like-arena-models-behaviours" aria-label="Bonus 2: Human annotators like arena model&rsquo;s behaviours">Bonus 2: Human annotators like arena model&rsquo;s behaviours</a></li></ul>
                </li>
                <li>
                    <a href="#4-reproducing-my-analysis" aria-label="4. Reproducing my analysis">4. Reproducing my analysis</a></li>
                <li>
                    <a href="#5-caveats" aria-label="5. Caveats">5. Caveats</a></li>
                <li>
                    <a href="#6-conclusion" aria-label="6. Conclusion">6. Conclusion</a></li>
                <li>
                    <a href="#further-links" aria-label="Further links">Further links</a></li>
                <li>
                    <a href="#acknowledgements" aria-label="Acknowledgements">Acknowledgements</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><style>
.faded { opacity: 0.6; }
</style>
<blockquote>
<p>üöß This post is currently a public draft with preliminary results. I appreciate any feedback &ndash; reach out via <code>contact [at] arduin.io</code>! üöß</p>
</blockquote>
<h2 id="1-introduction">1. Introduction<a hidden class="anchor" aria-hidden="true" href="#1-introduction">#</a></h2>
<p>The open-weights model <em>Llama 4 Maverick</em> was released on 5 April 2025. Around the same time, a related but non-identical experimental model version was evaluated on <a href="https://lmarena.ai/">Chatbot Arena</a> (<code>Llama-4-Maverick-03-26-Experimental</code>). Some users reported that these two models appear to have notable differences. In this post, I use our <a href="https://github.com/rdnfn/feedback-forensics">Feedback Forensics</a> app to quantitatively dissect <em>how exactly</em> the chat behaviour of the <em>public</em> and the <em>arena</em> models of Llama 4 Maverick differ.</p>
<h2 id="2-setup">2. Setup<a hidden class="anchor" aria-hidden="true" href="#2-setup">#</a></h2>
<blockquote>
<p><strong>‚úçÔ∏é Note on naming:</strong> for brevity, I will refer to the two versions of Llama 4 Maverick as the <strong>public model</strong> (used for open-weights release) and <strong>arena model</strong> (used on Chatbot Arena, full name: <code>Llama-4-Maverick-03-26-Experimental</code>) respectively.</p>
</blockquote>
<p>I don&rsquo;t have direct access to the <em>arena</em> model, but the Chatbot Arena team (very helpfully) released a <a href="https://huggingface.co/spaces/lmarena-ai/Llama-4-Maverick-03-26-Experimental_battles/tree/main/data">dataset</a> of responses generated by it. With <a href="https://github.com/rdnfn/feedback-forensics">Feedback Forensics</a> we can use this data to directly compare the arena model&rsquo;s behaviour to the <em>public</em> model&rsquo;s &mdash; without requiring new responses by the no longer accessible arena model itself (as conventional benchmarks would).</p>
<p>To compare the models, I run through the following steps:</p>
<ol>
<li>
<p><strong>Generation.</strong>  Using the <em>public</em> model, I first generate a set of responses to the same prompts for which we have responses from the <em>arena</em> model. The generated public model response are created via <a href="https://lambda.ai/">Lambda</a> and <a href="https://openrouter.ai/">OpenRouter</a>. Thus, we now have a dataset with three model responses* per prompt: (1) by the arena model, (2) by the public model, and (3) by an opposing (non-Llama-4) model from Chatbot Arena (e.g. GPT-4o).</p>
</li>
<li>
<p><strong>Behaviour annotation.</strong> Next, for any pair of responses to a prompt, I annotate (with the help of AI annotators) how the observed model behaviours differ across these responses, e.g. which of the responses is more polite. In particular, I test for the 40 different model behaviours included in the <a href="https://github.com/rdnfn/icai/blob/68e97f5078c313c6b8c31a8119ffc8b0cf40bc3c/src/inverse_cai/experiment/config/default_principles.py#L102">ICAI standard set of principles</a> (v3). This ICAI standard set is a collection of instructions (referred to as &ldquo;principles&rdquo;) for AI annotators to select for model behaviour differences. The collection includes behaviour differences that were previously observed in <a href="https://x.com/lmarena_ai/status/1909397817434816562">online discussions</a>, <a href="https://arxiv.org/abs/2410.12851">related literature</a> or detected via <a href="https://github.com/rdnfn/icai">Inverse Constitutional AI</a> (ICAI). I run principle-following AI annotators on all possible response pairs on a given prompt (e.g. model X&rsquo;s vs model Y&rsquo;s response) to collect information about how the models differ <em>relative to each other</em>. In the end, we have an extensive collection of annotated response pairs.</p>
<blockquote>
<p><strong>Illustrative example of the resulting dataset</strong>: a datapoint in this collection would be a (1) <em>prompt</em> (<code>&quot;How far away is the moon?&quot;</code>) and (2) a <em>pair of model responses</em> (arena model: <code>&quot;Great question, very far away: about 384k km&quot;</code>, public model: <code>&quot;About 384k km&quot;</code>) plus (3) 40 different <em>behaviour annotations</em> labelling which of the responses is more enthusiastic and polite (<code>arena</code>), or concise (<code>public</code>).</p>
</blockquote>
</li>
<li>
<p><strong>Visualisation.</strong> Finally, I locally run the Feedback Forensics app to look at the aggregate annotations results, allowing me to investigate how the behaviour of different models differs <em>across all available prompts and responses</em>. This analysis provides us with a quantitative evaluation of the entire observed model behaviour rather than relying on the small number of individual response pairs I would be able to manually inspect.</p>
</li>
</ol>
<blockquote>
<p>*<small><em>Some conversations (13.1%) from the Chatbot Arena dataset are multi-turn. For simplicity, I only generated responses for single-turn prompts (86.9% of prompts) with the public model. Thus, for some prompts we only have two responses: by the arena model and the opposing (non-Llama-4) model.</em></small></p>
</blockquote>
<h3 id="metrics">Metrics<a hidden class="anchor" aria-hidden="true" href="#metrics">#</a></h3>
<p>Throughout this analysis, we primarily rely on a measure of annotator performance we refer to as <em>annotator strength</em> or simply <em><strong>strength</strong></em>. In our scenario, annotator <em>strength</em> quantifies how much more a particular <em>selected</em> model exhibits a certain behaviour compared to the other models in our dataset (e.g. model X is more polite than the other models). This <em>relative behaviour</em> is measured by testing how consistently a principle-following annotator (e.g. an annotator selecting more polite responses) picks out the <em>selected</em> model&rsquo;s responses across the whole set of response pairs from the two (or more) models being compared. If such an annotator always picks the selected model, this indicates that the corresponding behaviour is stronger in the selected model <em>compared</em> to the other models. In other words: <em>strength measures how much selecting for a particular behaviour differentiates the selected model from the others</em>.</p>
<p>Numerically, <em>strength</em> is defined as the <a href="https://en.wikipedia.org/wiki/Cohen's_kappa">Cohen&rsquo;s kappa</a> agreement-level measure of two annotators, a principle-following (e.g. selecting the more polite response) and a model-selecting (e.g. always picking the arena model), weighted by the <em>relevance</em> of the principle-following annotator (e.g. the proportion of comparisons where the principle-following annotator decided that the principle applies).</p>
<p>In the context of this analysis, <strong>the strength measure can be interpreted as follows:</strong></p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Strength¬†metric</th>
          <th style="text-align: left">Interpretation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>close¬†to¬†+1</strong></td>
          <td style="text-align: left"><em>The selected model&rsquo;s responses exhibit a certain behaviour <strong>more</strong> than other models.</em><br><div style="margin-top: 7px;"></div>Explanation: the behaviour associated with the principle annotator seems to separate model X from other models, as an annotator following exclusively this principle is able to separate the model from the others.</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>close¬†to¬†0</strong></td>
          <td style="text-align: left"><em>The selected model&rsquo;s responses are <strong>similar</strong> to other models with respect to the annotated behaviour.</em> <br><div style="margin-top: 7px;"></div>Explanation: all compared models similarly exhibited a certain behaviour with similar frequency &mdash; meaning that the principle-following annotator could not distinguish between models.</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>close¬†to¬†-1</strong></td>
          <td style="text-align: left"><em>The selected model&rsquo;s responses exhibit a certain behaviour <strong>less</strong> than other models.</em> <br><div style="margin-top: 7px;"></div>Explanation: other models exhibited the principle&rsquo;s behaviour more than the selected model, the principle-following annotator selected those models more than the model of our model-selecting annotator.</td>
      </tr>
  </tbody>
</table>
<h2 id="3-results-whats-different-about-the-arena-model">3. Results: What&rsquo;s different about the arena model?<a hidden class="anchor" aria-hidden="true" href="#3-results-whats-different-about-the-arena-model">#</a></h2>
<p>Using this setup, we are now able to get a good overview of how the <em>arena</em> version of Llama 4 Maverick differs from the <em>public</em> version. In this section, I go through the main observations.</p>



<figure style="text-align: center;"><img
         src="img/00_ff_results_v2.png" width="500px"style="display:inline"
    /><figcaption>
        <p align="center"> Figure 1: Overview of the Feedback Forensics results. This is a screenshot of the <a href="https://app.feedbackforensics.com/?data=arena_special&ann_cols=llama4maverickexpvspublic">online app</a>.</p>
    </figcaption>
</figure>
<h3 id="first-and-most-obvious-responses-are-more-verbose">First and most obvious: Responses are more verbose<a hidden class="anchor" aria-hidden="true" href="#first-and-most-obvious-responses-are-more-verbose">#</a></h3>
<p>The arena model&rsquo;s responses appear more verbose than the public model&rsquo;s. The results in Figure 2 indicate that the arena model generated responses that were <em>&ldquo;too long&rdquo;</em>, <em>&ldquo;more verbose&rdquo;</em> and <em>&ldquo;provide more detailed explanations&rdquo;</em>, relative to the public model&rsquo;s responses.</p>



<figure style="text-align: center;"><img
         src="img/02_ff_annotator_length.png" width="900px"style="display:inline"
    /><figcaption>
        <p align="center"> Figure 2: Results showing that the arena model's responses are more verbose than the public model's. The numbers shown are <i>annotator strength</i> values, see <a href="#metrics">metrics section</a> for a detailed discussion.<br>(<a href="https://app.feedbackforensics.com/?data=arena_special&ann_cols=llama4maverickexpvspublic">online version of results</a>)</p>
    </figcaption>
</figure>
<p>This difference in length is also visible in Feedback Forensics&rsquo; <em>Overall Metrics</em> table (Figure 3), which shows more conventional annotation metrics and statistics. When measuring character length, arena model‚Äôs responses are longer relative to the public version for 98.6% of comparisons. The arena model&rsquo;s responses are about 6978.4 characters long on average vs 2981.9 for the public model. Note that differences in sampling procedures may play some role in this length difference, but similar differences can also be seen when looking at why the human annotations preferred the arena version in the original dataset (see Figure 7). Whether due to sampling procedures or not, the arena model is rather verbose &ndash; also relative to other models on Chatbot Arena.</p>



<figure style="text-align: center;"><img
         src="img/01_ff_basic_length.png" width="900px"style="display:inline"
    /><figcaption>
        <p align="center"> Figure 3: Screenshot of Feedback Forensics' <i>Overall Metrics</i> table with more conventional metrics showing how much longer the arena model's responses were relative to the public model's. (<a href="https://app.feedbackforensics.com/?data=arena_special&ann_cols=llama4maverickexpvspublic">online version</a>). </p>
    </figcaption>
</figure>
<h3 id="second-tone---friendlier-more-enthusiastic-more-humour">Second: Tone - friendlier, more enthusiastic, more humour<a hidden class="anchor" aria-hidden="true" href="#second-tone---friendlier-more-enthusiastic-more-humour">#</a></h3>
<p>The results show that the <em>tone</em> of the arena model appears to be quite different from the public version: much <em>friendlier</em>, more <em>confident</em>, <em>humorous</em>, <em>emotional</em>, <em>enthusiastic</em> and <em>casual</em> responses. Figure 4 shows the results for a subset of these behaviours - the full set can be found in the <a href="https://app.feedbackforensics.com/?data=arena_special&amp;ann_cols=llama4maverickexpvspublic">online version</a>.</p>



<figure style="text-align: center;"><img
         src="img/03_ff_tone.png" width="900px"style="display:inline"
    /><figcaption>
        <p align="center"> Figure 4: The arena model appears friendlier, more emotionally expressive, enthusiastic  and humorous. Further tone related behaviours were detected but are omitted here to save space, see <a href="https://app.feedbackforensics.com/?data=arena_special&ann_cols=llama4maverickexpvspublic">online version</a> for full results. The numbers shown are <i>annotator strength</i> values, see <a href="#metrics">metrics section</a> for a detailed discussion.</p>
    </figcaption>
</figure>
<h3 id="third-more-formatting-and-emojis">Third: More formatting and emojis<a hidden class="anchor" aria-hidden="true" href="#third-more-formatting-and-emojis">#</a></h3>
<p>Shown in Figure 5, the results indicate that the arena model more Markdown formatting than the public model: more <em>italics</em> and <em>bold</em> style, more <em>number lists</em>, and more <em>emojis</em>. These observations echo the <a href="https://x.com/lmarena_ai/status/1909397817434816562">online observations</a> highlighted by the Chatbot Arena team.</p>



<figure style="text-align: center;"><img
         src="img/04_ff_formatting.png" width="900px"style="display:inline"
    /><figcaption>
        <p align="center">Figure 5: the arena model uses more formatting than the public model. (<a href="https://app.feedbackforensics.com/?data=arena_special&ann_cols=llama4maverickexpvspublic">online version</a>) </p>
    </figcaption>
</figure>
<h3 id="bonus-1-things-that-stayed-consistent">Bonus 1: Things that stayed consistent<a hidden class="anchor" aria-hidden="true" href="#bonus-1-things-that-stayed-consistent">#</a></h3>
<p>I also find that some behaviour is similar between the models: on the limited set of prompts used in my experiments, the public and arena models are similarly very unlikely to suggest illegal activities, be offensive or use inappropriate language. The shown numbers are strength, but when <a href="https://app.feedbackforensics.com/?data=arena_special&amp;ann_cols=llama4maverickexpvspublic&amp;metric=relevance">looking at the relevance</a> of these principles we observe that each is only relevant on 3% or less of the dataset, i.e. neither model frequently produces such content on the tested prompt set.</p>



<figure style="text-align: center;"><img
         src="img/05_ff_exceptions.png" width="900px"style="display:inline"
    /><figcaption>
        <p align="center"> Figure 6: The arena and public version are similarly unlikely to produce offensive or inappropriate language, or suggest illegal activities <i>(on the tested prompt set)</i>. (<a href="https://app.feedbackforensics.com/?data=arena_special&ann_cols=llama4maverickexpvspublic">online version</a>)</p>
    </figcaption>
</figure>
<h3 id="bonus-2-human-annotators-like-arena-models-behaviours">Bonus 2: Human annotators like arena model&rsquo;s behaviours<a hidden class="anchor" aria-hidden="true" href="#bonus-2-human-annotators-like-arena-models-behaviours">#</a></h3>
<p>Feedback Forensics can not only be used to test <em>model behaviour</em>, but also to see <em>what model behaviour (human) annotators prefer</em>. In this scenario, the strength metric measures how well the principle-following annotators are able to <em>reconstruct</em> the human annotations. In this scenario, a high strength value can be interpreted as meaning that the annotations appear to prefer this model characteristic (or another characteristic that is correlated). For this analysis, I use the original non-tie human annotations from the chatbot Arena dataset (1,774 data points), comparing the arena model to other non-Llama4 models. The results shown below indicate that human annotators indeed like many of these changes in the arena version, potentially explaining the high performance of the arena model.</p>



<figure style="text-align: center;"><img
         src="img/06_ff_human.png" width="900px"style="display:inline"
    /><figcaption>
        <p align="center">Figure 7: Humans annotations indeed appear to select for many of the behaviours stronger in the arena model: friendlier tone, longer, and well-formatted (<a href="https://app.feedbackforensics.com/?data=arena_special&ann_cols=human">online version</a>).</p>
    </figcaption>
</figure>
<h2 id="4-reproducing-my-analysis">4. Reproducing my analysis<a hidden class="anchor" aria-hidden="true" href="#4-reproducing-my-analysis">#</a></h2>
<p>There are only three main steps involved in getting to this results locally (after installing <a href="https://github.com/rdnfn/icai">ICAI</a> and <a href="https://github.com/rdnfn/feedback-forensics">Feedback Forensics</a>):</p>
<ol>
<li>Parsing the data into a suitable format: use my parsing notebook <a href="https://github.com/rdnfn/feedback-forensics/blob/main/notebooks/01_llama4maverick_experimental_dataprep.ipynb">here</a>.</li>
<li>Running the relevant ICAI experiment with the standard principle annotators, using a <a href="https://github.com/rdnfn/icai/blob/main/exp/configs/322_arena_llama4_full_pairwise/config.yaml">config like this one</a>:
<pre tabindex="0"><code>icai-exp -cd exp/configs/322_arena_llama4_full_pairwise
</code></pre></li>
<li>Launching the Feedback Forensics app to inspect the results (this command is also included at the end of the output of the command above):
<pre tabindex="0"><code>feedback-forensics -d &lt;PATH-TO-RESULTS&gt;
</code></pre></li>
</ol>
<h2 id="5-caveats">5. Caveats<a hidden class="anchor" aria-hidden="true" href="#5-caveats">#</a></h2>
<p>This post describes preliminary analysis. The usual caveats for AI annotators and potentially inconsistent sampling procedures apply.</p>
<ul>
<li><strong>Public model sampling:</strong> I use OpenRouter/Lambda&rsquo;s default parameters other than setting temperature to 0. The exact sampling procedure likely differs from the one originally used for the arena model. However, for most relative behavioural observations discussed do not only hold against the public Maverick model &mdash; but also all the opponent models included in the original dataset.</li>
<li><strong>AI annotators:</strong> All our annotations are created by LLM-as-a-Judge annotator which can be noisy and have been show to exhibit various biases in different situations. I would expect the exact numbers to vary between annotation runs but the general trends to be the same (e.g. no sign flips in strength for currently high strength results). Nevertheless: it&rsquo;s always good advice to also look at (at least a random subset of) the data directly as well!</li>
</ul>
<h2 id="6-conclusion">6. Conclusion<a hidden class="anchor" aria-hidden="true" href="#6-conclusion">#</a></h2>
<p>Llama 4 Mavericks‚Äôs arena version is notably different from the public open-weight version. It shows many model behaviours that human annotators are known to prefer disproportionately, such as longer responses, more confident responses, and better formatted responses. The strong difference between the two models highlights the importance of having a detailed understanding of preference data beyond a single aggregate number ‚Äì our open-source <a href="https://github.com/rdnfn/feedback-forensics">Feedback Forensics</a> app aims to help with that!</p>
<h2 id="further-links">Further links<a hidden class="anchor" aria-hidden="true" href="#further-links">#</a></h2>
<p>If you want to understand your own model and data better, try Feedback Forensics!</p>
<ul>
<li>Feedback Forensics:
<ul>
<li>Online version: <a href="https://app.feedbackforensics.com/">app.feedbackforensics.com</a></li>
<li>Local version: <code>pip install feedback-forensics</code></li>
<li>GitHub &amp; docs: <a href="https://github.com/rdnfn/feedback-forensics">github.com/rdnfn/feedback-forensics</a></li>
</ul>
</li>
<li>Resources for reproducing results:
<ul>
<li>Chatbot Arena Llama 4 Maverick data: <a href="https://huggingface.co/spaces/lmarena-ai/Llama-4-Maverick-03-26-Experimental_battles/tree/main/data">link to HuggingFace</a></li>
<li>My data parsing notebook: <a href="https://github.com/rdnfn/feedback-forensics/blob/main/notebooks/01_llama4maverick_experimental_dataprep.ipynb">link to file on GitHub</a></li>
<li>My experiment configuration for running ICAI annotators: <a href="https://github.com/rdnfn/icai/blob/main/exp/configs/322_arena_llama4_full_pairwise/config.yaml">link file on GitHub</a></li>
</ul>
</li>
</ul>
<p>ü¶ù</p>
<h2 id="acknowledgements">Acknowledgements<a hidden class="anchor" aria-hidden="true" href="#acknowledgements">#</a></h2>
<p>I would like to thank <a href="https://sorellana.io/">Sofia Orellana</a> for detailed feedback on earlier versions of this post. All mistakes remain my own of course.</p>


    



















<h2>Citation</h2>

<p>If you found this post useful for your work, please consider citing it as:

    <blockquote>
<p>Findeis, Arduin. (Apr 2025). What exactly was different about the Chatbot Arena version of Llama 4 Maverick?. Retrieved from <a href="https://arduin.io/blog/llama4-analysis/">https://arduin.io/blog/llama4-analysis/</a>.</p>
</blockquote>

    or

    <pre tabindex="0"><code> @article{Findeis2023WhatexactlywasdifferentabouttheChatbotArenaversionofLlama 4 Maverick?,
        title = &#34;What exactly was different about the Chatbot Arena version of Llama 4 Maverick?&#34;,
        author = &#34;Findeis, Arduin&#34;,
        journal = &#34;arduin.io&#34;,
        year = &#34;2025&#34;,
        month = &#34;April&#34;,
        url = &#34;https://arduin.io/blog/llama4-analysis/&#34;
 } 
</code></pre>

</p>

    
  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>
        
        <a href="https://arduin.io/"><img src="/favicon-32x32.png" alt="logo" style="width: 20px; margin: 0 auto;opacity:0.6;"></a>
    </span>
    <span>&copy; 2025 <a href="https://arduin.io/">arduin.io</a></span>
    <span>
        - powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a> with <a href='https://arduin.io/papermod-tweaks/' rel="noopener" target="_blank">tweaks</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script src="/js/table_beautify.js"></script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
